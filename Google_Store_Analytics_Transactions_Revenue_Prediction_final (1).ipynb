{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3pbMFJ_7qSe"
      },
      "source": [
        "# Exploratory Data Analysis : Google Store Revenue Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTeyYlXz7qSj"
      },
      "source": [
        "**data dictionary**\n",
        "https://support.google.com/analytics/answer/3437719?hl=en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F44VmTE77qSk"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pickle\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn import preprocessing\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAc3MTK7qSk"
      },
      "outputs": [],
      "source": [
        "# import warnings\n",
        "\n",
        "# We do this to ignore several specific Pandas warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqWUUCjc7qSk"
      },
      "source": [
        "### Dataset Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig7m0Rhi7qSk"
      },
      "outputs": [],
      "source": [
        "# read the data\n",
        "train_df = pd.read_pickle('2017_clean.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhhB0QQG7qSk"
      },
      "outputs": [],
      "source": [
        "#train_df.shape\n",
        "print(\"Datasets contains %s of features, and %s of rows.\"%(train_df.shape[1],train_df.shape[0]))\n",
        "print(\"Number of unique visitors in train set : \",train_df.fullVisitorId.nunique(), \" out of rows : \",train_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtsR8wqw7qSl"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRFm_Yy-7qSl"
      },
      "outputs": [],
      "source": [
        "# understand the quantity of misisng values\n",
        "missing_values_count = train_df.isnull().sum().sort_values(ascending=False)\n",
        "missing_values_per = round((missing_values_count/len(train_df)) * 100,2)\n",
        "pd.concat([missing_values_count, missing_values_per], axis=1, keys = ['Quant Missing values', 'Percentage Missing values - %'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1tyl_oo7qSl"
      },
      "source": [
        "* In terms of of total revenue, about 1.2%of customers is generating the total revenues\n",
        "* More missing values related to traffic source, might need to dig more into it.\n",
        "* Some of NA (missing values) also contains values based on data dictionary. EX: Binary (1 and Na is 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMOHu-rW7qSl"
      },
      "source": [
        "**Change dataset into right format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLiSUovI7qSl"
      },
      "outputs": [],
      "source": [
        "# library of datetime\n",
        "from datetime import datetime\n",
        "\n",
        "# This function is to extract date, weekdays, day,month and visitHour features\n",
        "def date_process(df):\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\") # seting the column as pandas datetime\n",
        "    df[\"_weekday\"] = df['date'].dt.weekday #extracting week day\n",
        "    df[\"_day\"] = df['date'].dt.day # extracting day\n",
        "    df[\"_month\"] = df['date'].dt.month # extracting day\n",
        "    df[\"_year\"] = df['date'].dt.year # extracting day\n",
        "    df['_visitHour'] = (df['visitStartTime'].apply(lambda x: str(datetime.fromtimestamp(x).hour))).astype(int)\n",
        "\n",
        "    return df #returning the df after the transformations\n",
        "train_df = date_process(train_df) #calling the function that we created above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIhFggc-7qSl"
      },
      "outputs": [],
      "source": [
        "#change dataset into right format, then we can also save some memory space\n",
        "#fill missing values(0,1/True,False)\n",
        "train_df['totals.bounces'] = train_df['totals.bounces'].fillna(0)\n",
        "train_df['totals.newVisits'] =train_df['totals.newVisits'].fillna(0)\n",
        "train_df['trafficSource.isTrueDirect'] =train_df['trafficSource.isTrueDirect'].fillna('False')\n",
        "train_df['trafficSource.adwordsClickInfo.slot'] = train_df['trafficSource.adwordsClickInfo.slot'].fillna('None')\n",
        "train_df['trafficSource.adwordsClickInfo.adNetworkType'] = train_df['trafficSource.adwordsClickInfo.adNetworkType'].fillna('Unknown')\n",
        "#train_df['totals.timeOnSite'] = train_df['totals.timeOnSite'].fillna(0)\n",
        "#train_df['totals.transactions'] = train_df['totals.transactions'].fillna(0)\n",
        "#train_df['totals.transactionRevenue'] = train_df['totals.transactionRevenue'].fillna(0.0)\n",
        "#train_df['totals.totalTransactionRevenue'] = train_df['totals.totalTransactionRevenue'].fillna(0)\n",
        "train_df['trafficSource.adwordsClickInfo.page'] = train_df['trafficSource.adwordsClickInfo.page'].fillna(0)\n",
        "train_df['totals.hits']=train_df['totals.hits'].fillna(0)\n",
        "train_df.loc[train_df['geoNetwork.city'] == \"(not set)\", 'geoNetwork.city'] = np.nan\n",
        "train_df['geoNetwork.city'].fillna(\"NaN\", inplace=True)\n",
        "#change format of dataset\n",
        "train_df['channelGrouping']=train_df['channelGrouping'].astype('category')\n",
        "train_df['device.browser']=train_df['device.browser'].astype('category')\n",
        "train_df['device.operatingSystem']=train_df['device.operatingSystem'].astype('category')\n",
        "train_df['device.isMobile']=train_df['device.isMobile'].astype('category')\n",
        "train_df['device.deviceCategory']=train_df['device.deviceCategory'].astype('category')\n",
        "train_df['geoNetwork.continent']=train_df['geoNetwork.continent'].astype('category')\n",
        "train_df['geoNetwork.subContinent']=train_df['geoNetwork.subContinent'].astype('category')\n",
        "train_df['geoNetwork.country']=train_df['geoNetwork.country'].astype('category')\n",
        "train_df['geoNetwork.region']=train_df['geoNetwork.region'].astype('category')\n",
        "train_df['geoNetwork.city']=train_df['geoNetwork.city'].astype('category')\n",
        "train_df['geoNetwork.networkDomain']=train_df['geoNetwork.networkDomain'].astype('category')\n",
        "train_df['totals.hits']=train_df['totals.hits'].astype('int')\n",
        "train_df['totals.pageviews']=train_df['totals.pageviews'].astype('float')\n",
        "train_df['totals.bounces']=train_df['totals.bounces'].astype('float')\n",
        "train_df['totals.newVisits']=train_df['totals.newVisits'].astype('float')\n",
        "train_df['totals.sessionQualityDim']=train_df['totals.sessionQualityDim'].astype('float')\n",
        "train_df['totals.timeOnSite']=train_df['totals.timeOnSite'].astype('float')\n",
        "train_df['totals.transactions']=train_df['totals.transactions'].astype('float')\n",
        "train_df['totals.transactionRevenue']=train_df['totals.transactionRevenue'].astype('float')\n",
        "train_df['totals.totalTransactionRevenue']=train_df['totals.totalTransactionRevenue'].astype('float')\n",
        "train_df['trafficSource.campaign'] =train_df['trafficSource.campaign'].astype('category')\n",
        "train_df['trafficSource.source']=train_df['trafficSource.source'].astype('category')\n",
        "train_df['trafficSource.medium']=train_df['trafficSource.medium'].astype('category')\n",
        "train_df['trafficSource.isTrueDirect']=train_df['trafficSource.isTrueDirect'].astype('category')\n",
        "train_df['trafficSource.adwordsClickInfo.slot']=train_df['trafficSource.adwordsClickInfo.slot'].astype('category')\n",
        "train_df['trafficSource.adwordsClickInfo.adNetworkType']=train_df['trafficSource.adwordsClickInfo.adNetworkType'].astype('category')\n",
        "train_df['trafficSource.adwordsClickInfo.page']=train_df['trafficSource.adwordsClickInfo.page'].astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "iFf2tQuj7qSl"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeB5Zmq67qSl"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsRcnpsq7qSm"
      },
      "source": [
        "**Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjdB94yE7qSm"
      },
      "source": [
        "**1) Transaction Revenue Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntgAh-9i7qSm"
      },
      "outputs": [],
      "source": [
        "# Revenue\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.subplot(1,2,1)\n",
        "ax = sns.distplot(np.log(train_df[train_df['totals.totalTransactionRevenue'] > 0][\"totals.totalTransactionRevenue\"] + 0.01), bins=40, kde=True)\n",
        "ax.set_xlabel('Transaction RevenueLog', fontsize=5)\n",
        "ax.set_ylabel('Distribuition', fontsize=5)\n",
        "ax.set_title(\"Distribuition of Revenue Log\", fontsize=10)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "gdf = train_df.groupby(\"fullVisitorId\")[\"totals.totalTransactionRevenue\"].sum().reset_index()\n",
        "plt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf[\"totals.totalTransactionRevenue\"].values)))\n",
        "plt.xlabel('Index', fontsize=5)\n",
        "plt.ylabel('Revenue value', fontsize=5)\n",
        "plt.title(\"Revenue Value Distribution\", fontsize=10)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18iIyP6Z7qSm"
      },
      "source": [
        "* log of transaction revenue distribution is normally distributed\n",
        "* The second graph shows minorities of customers produce majorties of benefits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV1IhXJY7qSm"
      },
      "source": [
        "***ChannelGrouping Analysis***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlMy3Scx7qSm"
      },
      "outputs": [],
      "source": [
        "count = train_df['channelGrouping'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "ax = sns.barplot(count.index, count.values, alpha=0.8)\n",
        "ax.set_xlabel('Type of channel')\n",
        "ax.set_ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZUXgrtq7qSm"
      },
      "source": [
        "* Most our customers are coming from Organic Search, then the next channel is from direct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu1WxIBo7qSm"
      },
      "source": [
        "The TOP 5 Grouping Channels represents 97% of customer units. Respectivelly:\n",
        "\n",
        "* TOP 1 => Organic Search - 42.99%\n",
        "* TOP 2 => Social - 24.39%\n",
        "* TOP 3 => Direct - 15.42%\n",
        "* TOP 4 => Referral - 11.89%\n",
        "* TOP 5 => Paid Search - 2.55%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEPHg-ei7qSm"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "sns.barplot(x='channelGrouping', y='totals.totalTransactionRevenue', data=train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOf6gAzW7qSm"
      },
      "source": [
        "* Display actually has the most revenue even though most our customer is from Organic Search\n",
        "* The second most is from direct, it is also the second most group channel that get more customers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgsMXVN7qSm"
      },
      "source": [
        "#### 1) Device related columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr_5ORE67qSm"
      },
      "source": [
        "***device.browser Analysis***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzeBvE717qSm"
      },
      "outputs": [],
      "source": [
        "crosstab_eda = pd.crosstab(index=train_df['channelGrouping'], normalize=True,\n",
        "                           # at this line, I am using the isin to select just the top 5 of browsers\n",
        "                           columns=train_df[train_df['device.browser'].isin(train_df['device.browser']\\\n",
        "                                                                            .value_counts()[:5].index.values)]['device.browser'])\n",
        "\n",
        "crosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n",
        "                 figsize=(14,7), # adjusting the size of graphs\n",
        "                 stacked=True)   # code to unstack\n",
        "plt.title(\"Channel Grouping % for which Browser\", fontsize=20) # seting the title size\n",
        "plt.xlabel(\"The Channel Grouping Name\", fontsize=18) # seting the x label size\n",
        "plt.ylabel(\"Count\", fontsize=18) # seting the y label size\n",
        "plt.xticks(rotation=0)\n",
        "plt.show() # rendering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZddYl2_D7qSm"
      },
      "source": [
        "* Majority of users is from Chrome,and the second popular browser is Safari."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REnNDHFO7qSm"
      },
      "source": [
        "***device.isMobile Analysis***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "q4SxRKUA7qSm"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='device.isMobile', y='totals.totalTransactionRevenue', data=train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMR34HeF7qSn"
      },
      "source": [
        "***Date***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "V3l24xFn7qSn"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x=\"_month\", y=\"totals.totalTransactionRevenue\",hue='device.deviceCategory',data=train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TQe5EIu7qSn"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x=\"_visitHour\", y=\"totals.totalTransactionRevenue\", data=train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rD53JKm7qSn"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x=\"_weekday\", y=\"totals.totalTransactionRevenue\", data=train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN99KnqP7qSn"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='_visitHour', hue='device.deviceCategory', data=train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0KL5q_d7qSn"
      },
      "outputs": [],
      "source": [
        "df_date2=pd.get_dummies(train_df['_weekday'],prefix='weekday',drop_first=True)\n",
        "#df_date2=df_date2.join(pd.get_dummies(df_clean['_visitHour'],prefix='hour',drop_first=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOrYOoe17qSn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scaler: scaler\n",
        "#scaler = StandardScaler()\n",
        "\n",
        "# Create a PCA instance: pca\n",
        "pca = PCA()\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "\n",
        "\n",
        "# Fit the pipeline to 'samples'\n",
        "pca.fit(df_date2)\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(pca.n_components_)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('PCA feature')\n",
        "plt.ylabel('cumulative variance')\n",
        "plt.xticks(features)\n",
        "plt.show()\n",
        "# PCA() i can tell him the number of the pca I wanna use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMjaUUnB7qSn"
      },
      "outputs": [],
      "source": [
        "pca.components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeYpzP4a7qSn"
      },
      "outputs": [],
      "source": [
        "df_date3=pd.get_dummies(train_df['_month'],drop_first=True,prefix='month')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wof1DL67qSo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scaler: scaler\n",
        "#scaler = StandardScaler()\n",
        "\n",
        "# Create a PCA instance: pca\n",
        "pca = PCA()\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "\n",
        "\n",
        "# Fit the pipeline to 'samples'\n",
        "pca.fit(df_date3)\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(pca.n_components_)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('PCA feature')\n",
        "plt.ylabel('cumulative variance')\n",
        "plt.xticks(features)\n",
        "plt.show()\n",
        "# PCA() i can tell him the number of the pca I wanna use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqkKmsjd7qSo"
      },
      "outputs": [],
      "source": [
        "df_date3=pd.get_dummies(train_df['_visitHour'],drop_first=True,prefix='hour')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "ZvqFtkEI7qSo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scaler: scaler\n",
        "#scaler = StandardScaler()\n",
        "\n",
        "# Create a PCA instance: pca\n",
        "pca = PCA()\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "\n",
        "\n",
        "# Fit the pipeline to 'samples'\n",
        "pca.fit(df_date3)\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(pca.n_components_)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('PCA feature')\n",
        "plt.ylabel('cumulative variance')\n",
        "plt.xticks(features)\n",
        "plt.show()\n",
        "# PCA() i can tell him the number of the pca I wanna use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3cVgyn47qSo"
      },
      "source": [
        "* We try to use pca for date, but it does not work very well for dimension reduction\n",
        "* Going with exploratory analysis results\n",
        "   * Creating new feature isTuesday, because people tends to make more transactions on Tuesday\n",
        "   * isApril becuase there is a spike when wen plot month with transactions revenue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5nziEC17qSo"
      },
      "source": [
        "***device.deviceCategory***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "J79Mkw2X7qSo"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x=\"device.deviceCategory\", y=\"totals.totalTransactionRevenue\", data=train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h23d3MFK7qSo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "# revenue\n",
        "count = train_df.groupby(['device.browser'])['totals.transactionRevenue'].agg(['sum'])\n",
        "#sns.barplot(x='device.isMobile', y='totals.totalTransactionRevenue', data=train_df)\n",
        "count.sort_values(['sum'], ascending=False)[:5].plot(kind='bar', fontsize=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIhVvNx-7qSo"
      },
      "outputs": [],
      "source": [
        "# device browser\n",
        "count = train_df.groupby('device.operatingSystem')['totals.transactionRevenue'].agg(['sum'])\n",
        "count.sort_values(['sum'], ascending=False)[:5].plot(kind='bar', fontsize=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbqb2KG27qSo"
      },
      "source": [
        "**2) GeoNetwork Columns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfj-QfLG7qSo"
      },
      "source": [
        "***geoNetwork.continent***   \n",
        "Only 6 groups, ready to use.\n",
        "\n",
        "Thoughts:\n",
        "1. Americas has most counts and non-zero revenue.\n",
        "2. Asia and Europe have high counts but low non-zero revenue.\n",
        "3. Mean revenue of Africa is high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufO1PhyS7qSo"
      },
      "outputs": [],
      "source": [
        "# continent\n",
        "cnt = train_df.groupby('geoNetwork.continent')['totals.totalTransactionRevenue'].agg(['size', 'count', 'mean'])\n",
        "cnt.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\n",
        "cnt = cnt.sort_values(by=\"count\", ascending=False)\n",
        "cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebeutwyX7qSo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,4))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.barh(cnt.index.values, cnt['count'])\n",
        "plt.title(\"Continent - Revenue Count\", fontsize=13)\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.barh(cnt.index.values, cnt['count of non-zero revenue'])\n",
        "plt.title(\"Continent - non-zero revenue\", fontsize=13)\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.barh(cnt.index.values, cnt['mean'])\n",
        "plt.title(\"Continent - Revenue Mean\", fontsize=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxPV_xRx7qSo"
      },
      "source": [
        "***geoNetwork.subContinent***   \n",
        "About 20 groups, ready to use.\n",
        "Similar pattern as continent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9KwAcfX7qSo"
      },
      "outputs": [],
      "source": [
        "# subcontinent\n",
        "scnt = train_df.groupby('geoNetwork.subContinent')['totals.totalTransactionRevenue'].agg(['size', 'count', 'mean'])\n",
        "scnt.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\n",
        "scnt = scnt.sort_values(by=\"count\", ascending=False)\n",
        "scnt.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-adqYgsd7qSo"
      },
      "source": [
        "***geoNetwork.country***   \n",
        "* United State has only 45% of counts but over 95% of the non-zero revenue. That's interesting. Other countries almost generate no revenue except Canada. But Canada also only generate 174 pieces of non-zero revenue out of 30k counts of visit.\n",
        "\n",
        "* We may consider drop this column and create a feature **is_US** (1 for country==US, 0 for country!=US)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qvxq0CQs7qSo"
      },
      "outputs": [],
      "source": [
        "# country\n",
        "ct = train_df.groupby('geoNetwork.country')['totals.totalTransactionRevenue'].agg(['size', 'count', 'mean'])\n",
        "ct.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\n",
        "ct = ct.sort_values(by=\"count\", ascending=False)\n",
        "\n",
        "ct.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm5JenVM7qSp"
      },
      "outputs": [],
      "source": [
        "train_df[train_df['geoNetwork.country']=='United States']['totals.totalTransactionRevenue'].count()/train_df['totals.totalTransactionRevenue'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8bcp7Lh7qSp"
      },
      "outputs": [],
      "source": [
        "len(train_df[train_df['geoNetwork.country']=='United States']['totals.totalTransactionRevenue'])/len(train_df['totals.totalTransactionRevenue'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f15C0JgP7qSp"
      },
      "outputs": [],
      "source": [
        "# plot top 10\n",
        "ct_10 = ct.head(10)\n",
        "\n",
        "plt.figure(figsize=(7,10))\n",
        "\n",
        "plt.subplot(3,1,1)\n",
        "plt.barh(ct_10.index.values, ct_10['count'])\n",
        "plt.title(\"Country - Revenue Count\", fontsize=13)\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "plt.barh(ct_10.index.values, ct_10['count of non-zero revenue'])\n",
        "plt.title(\"Country - non-zero revenue\", fontsize=13)\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.barh(ct_10.index.values, ct_10['mean'])\n",
        "plt.title(\"Country - Revenue Mean\", fontsize=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMU8sMd97qSp"
      },
      "source": [
        "***geoNetwork.region***   \n",
        "* For non-zero revenue, region not available is nearly 40%. Among the remaining, California is the most and is above 30%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXQWn2aC7qSp"
      },
      "outputs": [],
      "source": [
        "# city\n",
        "city = train_df.groupby('geoNetwork.city')['totals.totalTransactionRevenue'].agg(['size', 'count', 'mean'])\n",
        "city.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\n",
        "city = city.sort_values(by=\"count\", ascending=False)\n",
        "\n",
        "city.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqdiz72B7qSp"
      },
      "outputs": [],
      "source": [
        "# plot top 20\n",
        "city_20 = city.head(20)\n",
        "\n",
        "plt.figure(figsize=(5,20))\n",
        "\n",
        "plt.subplot(3,1,1)\n",
        "plt.barh(city_20.index.values, city_20['count'])\n",
        "plt.title(\"Continent - Revenue Count\", fontsize=13)\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "plt.barh(city_20.index.values, city_20['count of non-zero revenue'])\n",
        "plt.title(\"Continent - non-zero revenue\", fontsize=13)\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.barh(city_20.index.values, city_20['mean'])\n",
        "plt.title(\"Continent - Revenue Mean\", fontsize=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFTbdEQa7qSp"
      },
      "source": [
        "***geoNetwork.networkDomain***   \n",
        "* About 29K domains, including unknown.unknown and (not set). It is interesting to find out that (not set) has better revenue than unknown.unknown.\n",
        "* Looks like this column is ready to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq5vAl_E7qSp"
      },
      "source": [
        "* Clean up networkDomain and just keep ending like edu,net,com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0nf1UaD7qSp"
      },
      "outputs": [],
      "source": [
        "train_df['domain'] = train_df['geoNetwork.networkDomain'].str.split('.')\n",
        "train_df['domain'] = [i[-1] for i in train_df['domain']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMJOO9Te7qSp"
      },
      "outputs": [],
      "source": [
        "# networkDomain\n",
        "dm = train_df.groupby('domain')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\n",
        "dm.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\n",
        "dm = dm.sort_values(by=\"count\", ascending=False)\n",
        "\n",
        "dm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi-EtpyK7qSp"
      },
      "outputs": [],
      "source": [
        "# plot top 20\n",
        "dm20 = dm.head(20)\n",
        "\n",
        "plt.figure(figsize=(5,20))\n",
        "\n",
        "plt.subplot(3,1,1)\n",
        "plt.barh(dm20.index.values, dm20['count'])\n",
        "plt.title(\"Domain - Revenue Count\", fontsize=13)\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "plt.barh(dm20.index.values, dm20['count of non-zero revenue'])\n",
        "plt.title(\"Domain - non-zero revenue\", fontsize=13)\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.barh(dm20.index.values, dm20['mean'])\n",
        "plt.title(\"Domain - Revenue Mean\", fontsize=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHIIcu0z7qSp"
      },
      "source": [
        "***Totals Columns Analysis***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuhTZC0b7qSp"
      },
      "outputs": [],
      "source": [
        "columns=['totals.hits',\n",
        " 'totals.pageviews',\n",
        " 'totals.sessionQualityDim',\n",
        " 'totals.timeOnSite',\n",
        " 'totals.transactions',\n",
        " 'totals.transactionRevenue',\n",
        " 'totals.totalTransactionRevenue']\n",
        "train_total=train_df[columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeRmxcH37qSp"
      },
      "outputs": [],
      "source": [
        "g = sns.pairplot(train_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWYYvQj67qSp"
      },
      "outputs": [],
      "source": [
        "def heatMap(df):\n",
        "    #Create Correlation df\n",
        "    corr = df.corr()\n",
        "    #Plot figsize\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    #Generate Color Map\n",
        "    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "    #Generate Heat Map, allow annotations and place floats in map\n",
        "    sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n",
        "    #Apply xticks\n",
        "    plt.xticks(range(len(corr.columns)), corr.columns);\n",
        "    #Apply yticks\n",
        "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
        "    #show plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcW9h-Rq7qSp"
      },
      "outputs": [],
      "source": [
        "heatMap(train_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eGIwbqn7qSq"
      },
      "source": [
        "* I think we can drop either total hits or page views since they share almost same information about customers and they have same correlation with revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmhF0a5s7qSq"
      },
      "outputs": [],
      "source": [
        "#sns.jointplot(x=train_df['totals.hits'], y=train_df['totals.transactionRevenue'],color ='g')\n",
        "g=sns.jointplot(x=np.log(train_df['totals.hits']), y=np.log(train_df['totals.totalTransactionRevenue']),kind=\"reg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp2LWL_37qSq"
      },
      "outputs": [],
      "source": [
        "g=sns.jointplot(x=np.log(train_df['totals.pageviews']), y=np.log(train_df['totals.totalTransactionRevenue']),kind=\"reg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhLNDSRJ7qSq"
      },
      "outputs": [],
      "source": [
        "g=sns.jointplot(train_df['totals.sessionQualityDim'], y=np.log(train_df['totals.totalTransactionRevenue']),kind=\"reg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENvu6arG7qSq"
      },
      "source": [
        "* Both page views and total hits is highly correlated to each other. After log transformation, with simple linear regressions. Both total hits and page views is positively related to log of transaction revenues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whRpTVWQ7qSq"
      },
      "outputs": [],
      "source": [
        "g=sns.jointplot(np.log(train_df['totals.timeOnSite']), y=np.log(train_df['totals.totalTransactionRevenue']),kind=\"reg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKZ364Aq7qSq"
      },
      "outputs": [],
      "source": [
        "tmp = train_df.groupby('fullVisitorId').size().value_counts().to_frame().reset_index()\n",
        "sum_ = tmp[tmp['index'].astype('int16') > 5][0].sum()\n",
        "tmp = tmp.head(5).append(pd.DataFrame({'index': ['more than 5'], 0: [sum_]})).reset_index()\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.title(\"Visits per user\")\n",
        "ax = sns.barplot(x=tmp['index'], y=tmp[0], palette='Blues_d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWj26G9W7qSq"
      },
      "source": [
        "***total.timeOnSite***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrTR0uOk7qSq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(range(train_df['totals.timeOnSite'].shape[0]), np.sort((train_df['totals.timeOnSite'].values)))\n",
        "plt.xlabel('index', fontsize=12)\n",
        "plt.ylabel('timeonSite', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "cnC7HObA7qSq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(train_df['totals.timeOnSite'], train_df['totals.totalTransactionRevenue'])\n",
        "plt.xlabel('Timeonside', fontsize=12)\n",
        "plt.ylabel('Revenue', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKIJCAta7qSq"
      },
      "source": [
        "***Traffic Source Analysis***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAEAQL-I7qSq"
      },
      "outputs": [],
      "source": [
        "traffic=['trafficSource.campaign',\n",
        " 'trafficSource.source',\n",
        " 'trafficSource.medium',\n",
        " 'trafficSource.keyword',\n",
        " 'trafficSource.referralPath',\n",
        " 'trafficSource.isTrueDirect',\n",
        " 'trafficSource.adContent',\n",
        " 'totals.transactionRevenue',\n",
        " 'totals.totalTransactionRevenue']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SZopeFV7qSq"
      },
      "outputs": [],
      "source": [
        "traffic_df=train_df[traffic]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_3_LbaI7qSq"
      },
      "outputs": [],
      "source": [
        "train_df['trafficSource.campaign'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgiYUhDc7qSq"
      },
      "source": [
        "* I think we can deal with this later. It looks very mass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtwgBt4z7qSq"
      },
      "outputs": [],
      "source": [
        "train_df['trafficSource.source'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZjizMxG7qSq"
      },
      "outputs": [],
      "source": [
        "# new = train_df['trafficSource.source'].str.split(\".\", n = 1, expand = True)\n",
        "# train_df['trafficSource.source1']=new[0]\n",
        "# train_df['trafficSource.source1']=new[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7XB2yl17qSr"
      },
      "source": [
        "Might need to do some further cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYR4FwW77qSr"
      },
      "outputs": [],
      "source": [
        "ax = sns.barplot(x=train_df['trafficSource.medium'], y=train_df['totals.totalTransactionRevenue'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alkAKRrB7qSr"
      },
      "outputs": [],
      "source": [
        "ax = sns.barplot(x=train_df['trafficSource.isTrueDirect'], y=train_df['totals.totalTransactionRevenue'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o-JO5kS7qSr"
      },
      "outputs": [],
      "source": [
        "data=train_df.groupby([\"trafficSource.keyword\"])[\"totals.totalTransactionRevenue\"].sum().sort_values(ascending=False)\n",
        "#sns.barplot(x=\"trafficSource.keyword\", y=\"totals.totalTransactionRevenue\", data=train_df)\n",
        "data[1:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWn3kjv77qSr"
      },
      "outputs": [],
      "source": [
        "train_df['trafficSource.referralPath'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXErgOnx7qSr"
      },
      "outputs": [],
      "source": [
        "train_df['trafficSource.adwordsClickInfo.page'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExbqzYem7qSr"
      },
      "outputs": [],
      "source": [
        " ax = sns.barplot(x=\"trafficSource.adwordsClickInfo.page\", y=\"totals.totalTransactionRevenue\", data=train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZopPfS5d7qSr"
      },
      "outputs": [],
      "source": [
        "train_df['trafficSource.keyword'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqrTfUL97qSr"
      },
      "outputs": [],
      "source": [
        "train_df['trafficSource.adContent'].value_counts()\n",
        "#dummy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XAPPsgP7qSr"
      },
      "outputs": [],
      "source": [
        "data=train_df.groupby([\"trafficSource.adContent\"])[\"totals.totalTransactionRevenue\"].sum().sort_values(ascending=False)\n",
        "data[1:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkdVvU-D7qSr"
      },
      "outputs": [],
      "source": [
        "train_df['trafficSource.adwordsClickInfo.gclId'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzy3oxdu7qSr"
      },
      "source": [
        "### Drop unnecessary columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP-XfcqS7qSr"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XJ_YyU57qSr"
      },
      "outputs": [],
      "source": [
        "df_clean=train_df.drop(['visitId','geoNetwork.region','geoNetwork.metro','geoNetwork.city','domain',\n",
        "                        'totals.transactions','totals.transactionRevenue','trafficSource.adwordsClickInfo.isVideoAd',\n",
        "                        'trafficSource.source','trafficSource.campaign','trafficSource.referralPath','visitNumber',\n",
        "                        'visitStartTime','trafficSource.keyword','trafficSource.adwordsClickInfo.gclId','_day','_year',\n",
        "                       'trafficSource.adContent'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX0t82Xf7qSr"
      },
      "outputs": [],
      "source": [
        "df_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C15gJrfs7qSr"
      },
      "source": [
        "### Fill NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWycLqut7qSr"
      },
      "outputs": [],
      "source": [
        "df_clean['totals.timeOnSite'] = train_df['totals.timeOnSite'].fillna(0)\n",
        "df_clean['totals.totalTransactionRevenue'] = train_df['totals.totalTransactionRevenue'].fillna(0.0)\n",
        "df_clean['totals.pageviews']=df_clean['totals.pageviews'].fillna(0)\n",
        "df_clean['totals.sessionQualityDim']=df_clean['totals.sessionQualityDim'].fillna(0)\n",
        "df_clean['totals.sessionQualityDim']=df_clean['totals.sessionQualityDim'].astype('category')\n",
        "df_clean['_weekday']=df_clean['_weekday'].astype('category')\n",
        "df_clean['_month']=df_clean['_month'].astype('category')\n",
        "df_clean['_visitHour']=df_clean['_visitHour'].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WqBT-4P7qSs"
      },
      "outputs": [],
      "source": [
        "df_clean.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6ljzB3f7qSs"
      },
      "outputs": [],
      "source": [
        "df_clean['month_4']=0\n",
        "df_clean.loc[df_clean['_month']==4,'month_4']=1\n",
        "df_clean['isTuesdays']=0\n",
        "df_clean.loc[df_clean['_weekday']==2,'isTuesdays']=1\n",
        "df_clean['BS_Firefox']=0\n",
        "df_clean.loc[df_clean['device.browser']=='Firefox','BS_Firefox']=1\n",
        "df_clean['BS_Chrome']=0\n",
        "df_clean.loc[df_clean['device.browser']=='Chrome','BS_Chrome']=1\n",
        "df_clean['BS_Safari']=0\n",
        "df_clean.loc[df_clean['device.browser']=='Safari','BS_Safari']=1\n",
        "df_clean['BS_IE']=0\n",
        "df_clean.loc[df_clean['device.browser']=='Internet Explorer','BS_IE']=1\n",
        "df_clean['BS_Android']=0\n",
        "df_clean.loc[df_clean['device.browser']=='Android Webview','BS_Android']=1\n",
        "\n",
        "\n",
        "df_clean['OS_Windows']=0\n",
        "df_clean.loc[df_clean['device.operatingSystem']=='Windows','OS_Windows']=1\n",
        "df_clean['OS_Macintosh']=0\n",
        "df_clean.loc[df_clean['device.operatingSystem']=='Macintosh','OS_Macintosh']=1\n",
        "df_clean['OS_Android']=0\n",
        "df_clean.loc[df_clean['device.operatingSystem']=='Android','OS_Android']=1\n",
        "df_clean['OS_iOS']=0\n",
        "df_clean.loc[df_clean['device.operatingSystem']=='iOS','OS_iOS']=1\n",
        "\n",
        "\n",
        "#subContinent1: Northern America (highest non-zero revenue)\n",
        "df_clean['subCont_NorthernAmerica']=0\n",
        "df_clean.loc[df_clean['geoNetwork.subContinent']=='Northern America','subCont_NorthernAmerica']=1\n",
        "\n",
        "#subContinent2: Western Africa (highest mean revenue)\n",
        "df_clean['subCont_Western Africa']=0\n",
        "df_clean.loc[df_clean['geoNetwork.subContinent']=='Western Africa','subCont_Western Africa']=1\n",
        "#country1: US (highest non-zero revenue)\n",
        "df_clean['country_USA']=0\n",
        "df_clean.loc[df_clean['geoNetwork.country']=='United States','country_USA']=1\n",
        "#country2: Australia (highest mean revenue)\n",
        "df_clean['country_Australia']=0\n",
        "df_clean.loc[df_clean['geoNetwork.country']=='Australia','country_Australia']=1\n",
        "\n",
        "#traffic medium\n",
        "df_clean['medium_cpm']=0\n",
        "df_clean.loc[df_clean['trafficSource.medium']=='cpm','medium_cpm']=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN8mQwgu7qSs"
      },
      "outputs": [],
      "source": [
        "#get dummies\n",
        "df_clean=df_clean.join(pd.get_dummies(df_clean['channelGrouping'])).drop(['(Other)'],axis = 1)\n",
        "df_clean=df_clean.join(pd.get_dummies(df_clean['_visitHour'],prefix='Hour')).drop(['Hour_1','Hour_4','Hour_5','Hour_6','Hour_7','Hour_16','Hour_17','Hour_18','Hour_19','Hour_20','Hour_21','Hour_22','Hour_23'],axis = 1)\n",
        "df_clean=df_clean.join(pd.get_dummies(df_clean['device.isMobile'],prefix='Mobile',drop_first=True))\n",
        "df_clean=df_clean.join(pd.get_dummies(df_clean['device.deviceCategory'])).drop(['tablet','mobile'],axis = 1)\n",
        "df_clean=df_clean.join(pd.get_dummies(df_clean['geoNetwork.continent'])).drop(['(not set)'],axis = 1)\n",
        "df_clean=df_clean.join(pd.get_dummies(df_clean['trafficSource.isTrueDirect'],prefix='isTrueDirect',drop_first=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cPY4tHX7qSs"
      },
      "outputs": [],
      "source": [
        "df_clean['sessionQuality.100'] = pd.get_dummies(df_clean['totals.sessionQualityDim'])[100]\n",
        "df_clean['Slot.RHS'] = pd.get_dummies(df_clean['trafficSource.adwordsClickInfo.slot'])['RHS']\n",
        "df_clean['Slot.Top'] = pd.get_dummies(df_clean['trafficSource.adwordsClickInfo.slot'])['Top']\n",
        "df_clean['Network.Content'] = pd.get_dummies(df_clean['trafficSource.adwordsClickInfo.adNetworkType'])['Content']\n",
        "df_clean['Network.GSearch'] = pd.get_dummies(df_clean['trafficSource.adwordsClickInfo.adNetworkType'])['Google Search']\n",
        "df_clean['Network.PSearch'] = pd.get_dummies(df_clean['trafficSource.adwordsClickInfo.adNetworkType'])['Search partners']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDfmIGwv7qSs"
      },
      "outputs": [],
      "source": [
        "df_clean=df_clean.drop(['_month','channelGrouping','_weekday','device.operatingSystem','geoNetwork.country',\n",
        "                       'geoNetwork.subContinent','geoNetwork.continent','geoNetwork.networkDomain','device.isMobile',\n",
        "                       'device.deviceCategory','device.browser','trafficSource.medium','trafficSource.isTrueDirect',\n",
        "                       '_visitHour','trafficSource.adwordsClickInfo.adNetworkType',\n",
        "              'trafficSource.adwordsClickInfo.slot',\n",
        "              'totals.sessionQualityDim'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c62veGvP7qSs"
      },
      "outputs": [],
      "source": [
        "df_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Y3ilYpBu7qSs"
      },
      "outputs": [],
      "source": [
        "df_clean.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWbRqupc7qSs"
      },
      "source": [
        "### Produce Transaction Level of Dataset for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYgw5HEX7qSs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkgIBuTS7qSs"
      },
      "outputs": [],
      "source": [
        "df_clean = df_clean.rename(columns={'totals.totalTransactionRevenue': 'revenue', 'fullVisitorId': 'id',\n",
        "                                    'trafficSource.adwordsClickInfo.page': 'TS_adwordsClickInfo.page',\n",
        "                                    'Affiliates': 'CG_Affiliates', 'Direct': 'CG_Direct','Display':'CG_Display',\n",
        "                                    'Organic Search': 'CG_organicSearch',\n",
        "                                   'Paid Search': 'CG_paidSearch' ,'Referral': 'CG_Referral',\n",
        "                                   'Social': 'CG_Social', 'Mobile_True': 'device_Mobile',\n",
        "                                   'desktop': 'device_Desktop', 'Africa': 'cont_Africa',\n",
        "                                   'Americas': 'cont_Americas', 'Asia': 'cont_Asia',\n",
        "                                   'Europe': 'cont_Europe', 'Oceania': 'cont_Oceania',\n",
        "                                   'isTrueDirect_False': 'TS_isTrueDirect', 'sessionQuality.100': 'TS_sessionQuality.100',\n",
        "                                   'Slot.RHS': 'TS_Slot.RHS', 'Slot.Top': 'TS_Slot.Top',\n",
        "                                    'Network.Content': 'TS_Network.Content', 'Network.GSearch': 'TS_Network.GSearch',\n",
        "                                    'Network.PSearch': 'TS_Network.PSearch'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMsHDPgE7qSs"
      },
      "outputs": [],
      "source": [
        "df_clean.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXnWIfN27qSs"
      },
      "outputs": [],
      "source": [
        "df_clean.to_pickle(\"trasaction_dataset.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sF5IDv57qSs"
      },
      "source": [
        "### Produce Aggregation to User level of Dataset for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62-qmHMM7qSs"
      },
      "outputs": [],
      "source": [
        "#customer dataframe cf\n",
        "cf1 = pd.DataFrame()  #float data\n",
        "cf2 = pd.DataFrame()  #dummy data\n",
        "cf3 = pd.DataFrame()  #target revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQly7sgD7qSs"
      },
      "outputs": [],
      "source": [
        "cf1 = df_clean.iloc[:,0:6]\n",
        "cf1 = cf1.groupby(['id']).agg(['sum','mean'])\n",
        "cf1.columns = ['_'.join(col).strip() for col in cf1.columns.values]\n",
        "#cf1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg3qO9-O7qSt"
      },
      "outputs": [],
      "source": [
        "select_columns = [1]\n",
        "select_columns.extend(range(8,57))\n",
        "cf2 = df_clean[df_clean.columns[select_columns]]\n",
        "cf2 = cf2.groupby(['id']).agg(['max'])\n",
        "cf2.columns = ['_'.join(col).strip() for col in cf2.columns.values]\n",
        "cf2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCvsaQwy7qSt"
      },
      "outputs": [],
      "source": [
        "cf3 = df_clean.groupby(['id'])['revenue'].agg(['sum'])\n",
        "cf3['logRevenue'] = np.log(1+cf3['sum'])\n",
        "cf3 = cf3.drop('sum', axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKnyyal77qSt"
      },
      "outputs": [],
      "source": [
        "# comebine customer dataset\n",
        "cf = pd.concat([cf3, cf1, cf2], axis=1)\n",
        "cf = cf.reset_index()\n",
        "cf = pd.DataFrame(cf)\n",
        "cf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8KMGc-a7qSt"
      },
      "outputs": [],
      "source": [
        "cf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8aRgiod7qSt"
      },
      "outputs": [],
      "source": [
        "# save to pickle\n",
        "cf.to_pickle(\"customer_dataset.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q2hgIzD7qSt"
      },
      "outputs": [],
      "source": [
        "cf = pd.read_pickle('customer_dataset.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99A3_P_G7qSt"
      },
      "outputs": [],
      "source": [
        "# drop time columns\n",
        "time_columns = ['month_4_max','isTuesdays_max','Hour_0_max','Hour_2_max','Hour_3_max','Hour_8_max','Hour_9_max','Hour_10_max','Hour_11_max','Hour_12_max','Hour_13_max','Hour_14_max','Hour_15_max']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOFLfssJ7qSt"
      },
      "outputs": [],
      "source": [
        "cf = cf.drop(time_columns, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEJBt03D7qSt"
      },
      "outputs": [],
      "source": [
        "cf.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQEJO7QT7qSt"
      },
      "source": [
        "### Graph Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcljftUE7qSt"
      },
      "outputs": [],
      "source": [
        "# import warnings\n",
        "import networkx as nx\n",
        "# We do this to ignore several specific Pandas warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# read the data\n",
        "Model_df = pd.read_pickle('trasaction_dataset.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ThFWVhm7qSt"
      },
      "outputs": [],
      "source": [
        "Model_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re293ZUB7qSt"
      },
      "outputs": [],
      "source": [
        "Model_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbwvrTmA7qSt"
      },
      "source": [
        "## Aggregated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CLXtvDN7qSt"
      },
      "outputs": [],
      "source": [
        "##Aggregating\n",
        "ID_df = Model_df.groupby(['id']).agg({'revenue':'sum'}).reset_index()\n",
        "ID_df['logRev'] = np.log1p(ID_df['revenue'])\n",
        "ID_df['logRev_t'] =round(ID_df['logRev'], -1)\n",
        "ID_df = ID_df.drop(['revenue', 'logRev'],axis = 1)\n",
        "ID_df1 = ID_df\n",
        "ID_df1['id_1'] = ID_df1['id']\n",
        "ID_df1 = ID_df1.drop('id',axis = 1)\n",
        "ID_df = ID_df.drop('id_1',axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdXY20WB7qSt"
      },
      "outputs": [],
      "source": [
        "ID_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHcwnlKT7qSu"
      },
      "outputs": [],
      "source": [
        "ID_df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcGSe00y7qSu"
      },
      "outputs": [],
      "source": [
        "ID_df1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrIURN3W7qSu"
      },
      "outputs": [],
      "source": [
        "joinme = pd.merge(ID_df1.head(100),\n",
        "                  ID_df.head(100), on='logRev_t', how='inner')\n",
        "joinme = joinme[joinme['id'] != joinme['id_1']]\n",
        "#joinme = ID_df.join(ID_df1, lsuffix='_1', rsuffix='_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4jUVhbN7qSu"
      },
      "outputs": [],
      "source": [
        "#greater = joinme[joinme['logRev_t' ] > 0 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LxM29ci7qSu"
      },
      "outputs": [],
      "source": [
        "#greater.head()\n",
        "joinme['logRev_t'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQIE_6JE7qSu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "FG = nx.from_pandas_edgelist(joinme, source='id',\n",
        "                             target='id_1', edge_attr=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGGW5pf37qSu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "nx.draw(FG, with_labels=False)\n",
        "plt.title()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdeURnX-7qSu"
      },
      "source": [
        "## Transaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUGz32_q7qSu"
      },
      "outputs": [],
      "source": [
        "## Not Aggregating\n",
        "Trans_df = Model_df[['id', 'revenue']]\n",
        "    # Getting rid of zeros\n",
        "Trans_df = Trans_df[Trans_df['revenue'] > 0]\n",
        "    # Smaller Values\n",
        "Trans_df['revenue'] = np.log1p(Trans_df['revenue'] )\n",
        "Trans_df['revenue'] =round(Trans_df['revenue'], -1)\n",
        "\n",
        "Trans_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hna5h2yB7qSu"
      },
      "outputs": [],
      "source": [
        "Trans_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efvK-sVp7qSu"
      },
      "outputs": [],
      "source": [
        "joinme = pd.merge(Trans_df.head(1000),\n",
        "                  Trans_df.head(1000), on='revenue', how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJld2YyY7qSu"
      },
      "outputs": [],
      "source": [
        "joinme = joinme[joinme['id_x'] != joinme['id_y']]\n",
        "joinme.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlCawy3m7qSu"
      },
      "outputs": [],
      "source": [
        "joinme.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miA3M4i57qSu"
      },
      "outputs": [],
      "source": [
        "TG = nx.from_pandas_edgelist(joinme, source='id_x',\n",
        "                             target='id_y', edge_attr=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH4OJcx97qSu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "nx.draw(TG, with_labels=False)\n",
        "plt.title('Transaction Revenue Graph')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "354bbiol7qSu"
      },
      "source": [
        "# Country Graph Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CO-7yXk7qSu"
      },
      "outputs": [],
      "source": [
        "# read the data\n",
        "Data_df = pd.read_pickle('2017_clean.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0umDQnD7qSv"
      },
      "outputs": [],
      "source": [
        "Data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55tzmEvK7qSv"
      },
      "outputs": [],
      "source": [
        "Data_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BGmqN5F7qSv"
      },
      "outputs": [],
      "source": [
        "country_df = Data_df.loc[:, ['fullVisitorId', 'totals.transactionRevenue', 'geoNetwork.country']]\n",
        "country_df\n",
        "country_df['totals.transactionRevenue'] = country_df['totals.transactionRevenue'].fillna(0)\n",
        "country_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzwPz73E7qSv"
      },
      "outputs": [],
      "source": [
        "this = country_df.loc[:,['fullVisitorId', 'geoNetwork.country']].groupby(['fullVisitorId','geoNetwork.country']).count()\n",
        "thisone = pd.DataFrame(data=this)\n",
        "thisone = thisone.reset_index().groupby(['fullVisitorId'])['geoNetwork.country'].count()\n",
        "thisone = pd.DataFrame(data=thisone)\n",
        "thisone.head()\n",
        "#newone.head()\n",
        "newone = thisone[thisone['geoNetwork.country'] > 1]\n",
        "#this.head()['fullVisitorId'].groupby(['fullVisitorId']).count()\n",
        "#this.fullVisitorId.value_counts().reset_index(name=\"count\").query(\"count > 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0V5zs_47qSv"
      },
      "outputs": [],
      "source": [
        "newone.shape\n",
        "newone.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chbg9c5B7qSv"
      },
      "outputs": [],
      "source": [
        "newone = newone.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWKEwUvF7qSv"
      },
      "outputs": [],
      "source": [
        "newone['fullVisitorId'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWTzFc0y7qSv"
      },
      "source": [
        "### Comparing Revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEh2wdhy7qSv"
      },
      "outputs": [],
      "source": [
        "joinme = pd.merge(newone['fullVisitorId'],\n",
        "                  country_df, on='fullVisitorId', how='left')\n",
        "joinme.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CXcxWXi7qSv"
      },
      "outputs": [],
      "source": [
        "joinme.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjlH0ZBd7qSv"
      },
      "outputs": [],
      "source": [
        "jjiin = pd.merge(joinme.loc[:,['fullVisitorId', 'totals.transactionRevenue']],\n",
        "                 joinme.loc[:,['fullVisitorId', 'totals.transactionRevenue']],\n",
        "                 on = 'totals.transactionRevenue', how = 'inner')\n",
        "jjiin.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNW8P11l7qSv"
      },
      "outputs": [],
      "source": [
        "jjiin.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A0JumMR7qSv"
      },
      "outputs": [],
      "source": [
        "TG = nx.from_pandas_edgelist(jjiin, source='fullVisitorId_x',\n",
        "                             target='fullVisitorId_y', edge_attr=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfOiHQdp7qSv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "nx.draw(TG, with_labels=True)\n",
        "plt.title('Multiple Countries: Revenue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SqEDqBcc7qSv"
      },
      "outputs": [],
      "source": [
        "Data_df[Data_df['fullVisitorId'] == '7745913892709272663']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "09s2jPKn7qSw"
      },
      "outputs": [],
      "source": [
        "joinme[joinme['fullVisitorId'] == '7745913892709272663']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwjCRRRH7qSw"
      },
      "source": [
        "### Comparing Country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhxCZXJv7qSw"
      },
      "outputs": [],
      "source": [
        "jjiin = pd.merge(joinme.loc[:,['fullVisitorId', 'geoNetwork.country']],\n",
        "                 joinme.loc[:,['fullVisitorId', 'geoNetwork.country']],\n",
        "                 on = 'geoNetwork.country', how = 'inner')\n",
        "jjiin.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrFZoz2L7qSw"
      },
      "outputs": [],
      "source": [
        "CG = nx.from_pandas_edgelist(jjiin, source='fullVisitorId_x',\n",
        "                             target='fullVisitorId_y', edge_attr=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8VkOn3W7qSw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "nx.draw(CG, with_labels=True)\n",
        "plt.title('Multiple Countries: Country')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkIp6fGm7qSw"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLIXx_LO7qSw"
      },
      "source": [
        "### Modeling on Transaction Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8iMTAds7qSw"
      },
      "outputs": [],
      "source": [
        "# read the data\n",
        "Model_df = pd.read_pickle('trasaction_dataset.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oduwNRkV7qSw"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import xgboost\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve,GroupShuffleSplit\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import explained_variance_score\n",
        "# Importing Classifier Modules\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "import sklearn.model_selection as cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qoCwOps7qSw"
      },
      "outputs": [],
      "source": [
        "#split train test datset\n",
        "#Split dataset with individual id instead of spliting it into transaction levels\n",
        "seed=3\n",
        "X = Model_df.drop(['revenue'],axis = 1)\n",
        "y = Model_df.loc[:,['id','revenue']]\n",
        "#np.log1p()\n",
        "train_inds,test_inds= next(GroupShuffleSplit(test_size=0.20,n_splits=2,random_state=3).split(X,groups=X['id']))\n",
        "X_train=X.loc[train_inds]\n",
        "X_test =X.loc[test_inds]\n",
        "y_train=y.loc[train_inds]\n",
        "y_test=y.loc[test_inds]\n",
        "\n",
        "\n",
        "y_train=np.log1p(y_train.drop(['id'],axis=1)).values[:,0]\n",
        "y_test=np.log1p(y_test.drop(['id'],axis=1)).values[:,0]\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20,random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W4Ileiv7qSw"
      },
      "outputs": [],
      "source": [
        "train_date=X_train['date']\n",
        "train_id=X_train['id']\n",
        "X_train=X_train.drop(['id','date'],axis=1)\n",
        "test_date=X_test['date']\n",
        "test_id=X_test['id']\n",
        "X_test=X_test.drop(['id','date'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXMT5IWB7qSw"
      },
      "source": [
        "Target, prediction process\n",
        "* 1st log1p to target\n",
        "* 2nd exmp1 predictions\n",
        "* 3rd sum predictions\n",
        "* 4th log1p to sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYVfbiyf7qSw"
      },
      "source": [
        "**Linear Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twiplk2D7qSw"
      },
      "outputs": [],
      "source": [
        "reg = LinearRegression()\n",
        "reg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t25AKPlS7qSw"
      },
      "source": [
        "**RMSE results for Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAscr2E_7qSw"
      },
      "outputs": [],
      "source": [
        "predictions = reg.predict(X_test)\n",
        "mse_dt = MSE(predictions, y_test)\n",
        "# Compute rmse_dt\n",
        "rmse_dt = mse_dt**(1/2)\n",
        "# Print rmse_dt\n",
        "print(\"Test set RMSE of dt: {}\".format(rmse_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyLJVHZt7qSw"
      },
      "outputs": [],
      "source": [
        "# Import mean_squared_error from sklearn.metrics as MSE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Predict the labels of the training set\n",
        "y_pred_train = reg.predict(X_train)\n",
        "\n",
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('Train RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO3vLfxT7qSx"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(reg, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
        "score_description = \" %0.5f (+/- %0.5f)\" % (np.sqrt(scores.mean()*-1), scores.std() * 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4o_yDwq7qSx"
      },
      "outputs": [],
      "source": [
        "print('{model:25} CV-5 RMSE: {score}'.format(\n",
        " model=reg.__class__.__name__,\n",
        " score=score_description\n",
        " ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXNeUWt77qSx"
      },
      "source": [
        "**Visual Presentation of Model results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NZMfLBG7qSx"
      },
      "outputs": [],
      "source": [
        "log_revenue_compare=pd.DataFrame({'date': train_date, 'log_revenue_pred': y_pred_train,'log_revenue_actual':y_train})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoZR57Kz7qSx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "ax = sns.lineplot(x=\"date\", y=\"log_revenue_pred\", data=log_revenue_compare,color='b')\n",
        "ax = sns.lineplot(x=\"date\", y=\"log_revenue_actual\", data=log_revenue_compare,color='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZXE0do47qSx"
      },
      "source": [
        "**RMSE after aggregating into user level**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av97ocGj7qSx"
      },
      "outputs": [],
      "source": [
        "y_pred_train[y_pred_train<0]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Aj-RhT87qSx"
      },
      "outputs": [],
      "source": [
        "revenue_per_user=pd.DataFrame({'user_id': np.array(train_id), 'revenue_pred': np.expm1(y_pred_train),'revenue_actual': np.expm1(y_train)})\n",
        "revenue_per_user=revenue_per_user.groupby(['user_id']).agg({'revenue_pred':'sum','revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user['log_sum_pred']=np.log1p(revenue_per_user['revenue_pred'])\n",
        "revenue_per_user['log_sum_actual']=np.log1p(revenue_per_user['revenue_actual'])\n",
        "revenue_per_user=revenue_per_user.drop(['revenue_pred','revenue_actual'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIqEEYIA7qSx"
      },
      "outputs": [],
      "source": [
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(revenue_per_user['log_sum_pred'], revenue_per_user['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('Train RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVQbQgWE7qSx"
      },
      "outputs": [],
      "source": [
        "predictions[predictions<0]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvFeVMzZ7qSx"
      },
      "outputs": [],
      "source": [
        "revenue_per_user_test=pd.DataFrame({'user_id': test_id, 'revenue_pred': np.expm1(predictions), 'revenue_actual': np.expm1(y_test)})\n",
        "revenue_per_user_test=revenue_per_user_test.groupby(['user_id']).agg({'revenue_pred':'sum','revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user_test['log_sum_pred']=np.log1p(revenue_per_user_test['revenue_pred'])\n",
        "revenue_per_user_test['log_sum_actual']=np.log1p(revenue_per_user_test['revenue_actual'])\n",
        "revenue_per_user_test=revenue_per_user_test.drop(['revenue_pred','revenue_actual'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArIzDkZK7qSx"
      },
      "outputs": [],
      "source": [
        "mse_dt = (MSE(revenue_per_user_test['log_sum_pred'], revenue_per_user_test['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print rmse_dt\n",
        "print(\"Test set RMSE of dt: {}\".format(rmse_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zRJ779-W7qSx"
      },
      "outputs": [],
      "source": [
        "###Cross validation for aggregate results\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Compute the array containing the 10-folds CV MSEs\n",
        "\n",
        "\n",
        "cross_val_pred = cross_val_predict(reg, X_train, y_train, cv=10,\n",
        "                                  groups = train_id,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "#RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "#print('CV RMSE: {}'.format(RMSE_CV))\n",
        "\n",
        "\n",
        "cross_val_pred[cross_val_pred<0] = 0\n",
        "\n",
        "\n",
        "revenue_per_user=pd.DataFrame({'user_id': train_id, 'revenue_pred': np.expm1(cross_val_pred),\n",
        "                               'revenue_actual': np.expm1(y_train)})\n",
        "revenue_per_user=revenue_per_user.groupby(['user_id']).agg({'revenue_pred':'sum',\n",
        "                                                            'revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user['log_sum_pred']=np.log1p(revenue_per_user['revenue_pred'])\n",
        "revenue_per_user['log_sum_actual']=np.log1p(revenue_per_user['revenue_actual'])\n",
        "revenue_per_user=revenue_per_user.drop(['revenue_pred','revenue_actual'],axis=1)\n",
        "\n",
        "\n",
        "################NEW BOX#################################################\n",
        "\n",
        "###TESTING NEW WAY\n",
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(revenue_per_user['log_sum_pred'], revenue_per_user['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('CV RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvXsCKLO7qSx"
      },
      "source": [
        "**XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-gUr7c77qSx"
      },
      "outputs": [],
      "source": [
        "xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.05, gamma=0, subsample=0.75,\n",
        "                           colsample_bytree=1, max_depth=5)\n",
        "\n",
        "\n",
        "\n",
        "xgb.fit(X_train,y_train)\n",
        "predictions = xgb.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVIj0lY67qSx"
      },
      "source": [
        "**RMSE results for Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcVfUE8v7qSy"
      },
      "outputs": [],
      "source": [
        "mse_dt = MSE(predictions, y_test)\n",
        "# Compute rmse_dt\n",
        "rmse_dt = mse_dt**(1/2)\n",
        "# Print rmse_dt\n",
        "print(\"Test set RMSE of dt: {}\".format(rmse_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI-IyGpF7qSy"
      },
      "outputs": [],
      "source": [
        "# Import mean_squared_error from sklearn.metrics as MSE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Predict the labels of the training set\n",
        "y_pred_train = xgb.predict(X_train)\n",
        "\n",
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('Train RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNazW9xd7qSy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "# Compute the array containing the 10-folds CV MSEs\n",
        "MSE_CV_scores = - cross_val_score(xgb, X_train, y_train, cv=10,\n",
        "                                  scoring='neg_mean_squared_error',\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "#print('CV RMSE: {}'.format(RMSE_CV))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLnyIxY87qSy"
      },
      "outputs": [],
      "source": [
        "print('CV RMSE: {}'.format(RMSE_CV))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duISdQr47qSy"
      },
      "source": [
        "**RMSE after aggregating into user level**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5XGTghE7qSy"
      },
      "outputs": [],
      "source": [
        "y_pred_train[y_pred_train<0]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG-gM6Ai7qSy"
      },
      "outputs": [],
      "source": [
        "revenue_per_user=pd.DataFrame({'user_id': train_id, 'revenue_pred': np.expm1(y_pred_train), 'revenue_actual': np.expm1(y_train)})\n",
        "revenue_per_user=revenue_per_user.groupby(['user_id']).agg({'revenue_pred':'sum','revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user['log_sum_pred']=np.log1p(revenue_per_user['revenue_pred'])\n",
        "revenue_per_user['log_sum_actual']=np.log1p(revenue_per_user['revenue_actual'])\n",
        "revenue_per_user=revenue_per_user.drop(['revenue_pred','revenue_actual'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oKUW55e7qSy"
      },
      "outputs": [],
      "source": [
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(revenue_per_user['log_sum_pred'], revenue_per_user['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('Train RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaN2wmfC7qSy"
      },
      "outputs": [],
      "source": [
        "predictions[predictions<0]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_icPyLw7qSy"
      },
      "outputs": [],
      "source": [
        "revenue_per_user_test=pd.DataFrame({'user_id': test_id, 'revenue_pred': np.expm1(predictions), 'revenue_actual': np.expm1(y_test)})\n",
        "revenue_per_user_test=revenue_per_user_test.groupby(['user_id']).agg({'revenue_pred':'sum','revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user_test['log_sum_pred']=np.log1p(revenue_per_user_test['revenue_pred'])\n",
        "revenue_per_user_test['log_sum_actual']=np.log1p(revenue_per_user_test['revenue_actual'])\n",
        "revenue_per_user_test=revenue_per_user_test.drop(['revenue_pred','revenue_actual'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFHa4ZsR7qSy"
      },
      "outputs": [],
      "source": [
        "mse_dt = (MSE(revenue_per_user_test['log_sum_pred'], revenue_per_user_test['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print rmse_dt\n",
        "print(\"Test set RMSE of dt: {}\".format(rmse_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH27V01U7qSy"
      },
      "outputs": [],
      "source": [
        "###Cross validation for aggregate results\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Compute the array containing the 10-folds CV MSEs\n",
        "\n",
        "\n",
        "cross_val_pred = cross_val_predict(xgb, X_train, y_train, cv=10,\n",
        "                                  groups = train_id,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "#RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "#print('CV RMSE: {}'.format(RMSE_CV))\n",
        "\n",
        "\n",
        "cross_val_pred[cross_val_pred<0] = 0\n",
        "\n",
        "\n",
        "revenue_per_user=pd.DataFrame({'user_id': train_id, 'revenue_pred': np.expm1(cross_val_pred),\n",
        "                               'revenue_actual': np.expm1(y_train)})\n",
        "revenue_per_user=revenue_per_user.groupby(['user_id']).agg({'revenue_pred':'sum',\n",
        "                                                            'revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user['log_sum_pred']=np.log1p(revenue_per_user['revenue_pred'])\n",
        "revenue_per_user['log_sum_actual']=np.log1p(revenue_per_user['revenue_actual'])\n",
        "revenue_per_user=revenue_per_user.drop(['revenue_pred','revenue_actual'],axis=1)\n",
        "\n",
        "\n",
        "################NEW BOX#################################################\n",
        "\n",
        "###TESTING NEW WAY\n",
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(revenue_per_user['log_sum_pred'], revenue_per_user['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('CV RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz0UvFwc7qSy"
      },
      "source": [
        "**Gradiant Boost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhF-Xadg7qSy"
      },
      "outputs": [],
      "source": [
        "from sklearn import ensemble\n",
        "# Fit regression model\n",
        "params = {'n_estimators': 500, 'max_depth': 5, 'min_samples_split': 2,\n",
        "          'learning_rate': 0.01, 'loss': 'ls'}\n",
        "gradient = ensemble.GradientBoostingRegressor(**params)\n",
        "\n",
        "gradient.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDajR0CK7qSz"
      },
      "source": [
        "**RMSE results for Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCZ_E-7M7qSz"
      },
      "outputs": [],
      "source": [
        "# Predict the test set labels 'y_pred'\n",
        "predictions = gradient.predict(X_test)\n",
        "y_pred_train=gradient.predict(X_train)\n",
        "\n",
        "# Evaluate the test set RMSE\n",
        "rmse_test = MSE(y_test, predictions)**(1/2)\n",
        "rmse_train = MSE(y_train, y_pred_train)**(1/2)\n",
        "\n",
        "print(\"Test set RMSE of dt: {}\".format(rmse_dt))\n",
        "print('Train RMSE: {}'.format(rmse_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e40hQxc-7qSz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "# Compute the array containing the 10-folds CV MSEs\n",
        "MSE_CV_scores = - cross_val_score(gradient, X_train, y_train, cv=10,\n",
        "                                  scoring='neg_mean_squared_error',\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "print('CV RMSE: {}'.format(RMSE_CV))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6WfwnWs7qSz"
      },
      "source": [
        "**RMSE after aggregating into user level**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24pg0Q-87qSz"
      },
      "outputs": [],
      "source": [
        "y_pred_train[y_pred_train<0]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB3XHldK7qSz"
      },
      "outputs": [],
      "source": [
        "revenue_per_user=pd.DataFrame({'user_id': train_id, 'revenue_pred': np.expm1(y_pred_train), 'revenue_actual': np.expm1(y_train)})\n",
        "revenue_per_user=revenue_per_user.groupby(['user_id']).agg({'revenue_pred':'sum','revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user['log_sum_pred']=np.log1p(revenue_per_user['revenue_pred'])\n",
        "revenue_per_user['log_sum_actual']=np.log1p(revenue_per_user['revenue_actual'])\n",
        "revenue_per_user=revenue_per_user.drop(['revenue_pred','revenue_actual'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYeWm5h57qSz"
      },
      "outputs": [],
      "source": [
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(revenue_per_user['log_sum_pred'], revenue_per_user['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('Train RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W189g9_v7qSz"
      },
      "outputs": [],
      "source": [
        "predictions[predictions<0]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upMhy8tZ7qSz"
      },
      "outputs": [],
      "source": [
        "revenue_per_user_test=pd.DataFrame({'user_id': test_id, 'revenue_pred': np.expm1(predictions), 'revenue_actual': np.expm1(y_test)})\n",
        "revenue_per_user_test=revenue_per_user_test.groupby(['user_id']).agg({'revenue_pred':'sum','revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user_test['log_sum_pred']=np.log1p(revenue_per_user_test['revenue_pred'])\n",
        "revenue_per_user_test['log_sum_actual']=np.log1p(revenue_per_user_test['revenue_actual'])\n",
        "revenue_per_user_test=revenue_per_user_test.drop(['revenue_pred','revenue_actual'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "BQxM3Rrw7qSz"
      },
      "outputs": [],
      "source": [
        "mse_dt = (MSE(revenue_per_user_test['log_sum_pred'], revenue_per_user_test['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print rmse_dt\n",
        "print(\"Test set RMSE of dt: {}\".format(rmse_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHx56kYL7qSz"
      },
      "outputs": [],
      "source": [
        "###Cross validation for aggregate results\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Compute the array containing the 10-folds CV MSEs\n",
        "\n",
        "\n",
        "cross_val_pred = cross_val_predict(gradient, X_train, y_train, cv=10,\n",
        "                                  groups = train_id,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "#RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "#print('CV RMSE: {}'.format(RMSE_CV))\n",
        "\n",
        "\n",
        "cross_val_pred[cross_val_pred<0] = 0\n",
        "\n",
        "\n",
        "revenue_per_user=pd.DataFrame({'user_id': train_id, 'revenue_pred': np.expm1(cross_val_pred),\n",
        "                               'revenue_actual': np.expm1(y_train)})\n",
        "revenue_per_user=revenue_per_user.groupby(['user_id']).agg({'revenue_pred':'sum',\n",
        "                                                            'revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user['log_sum_pred']=np.log1p(revenue_per_user['revenue_pred'])\n",
        "revenue_per_user['log_sum_actual']=np.log1p(revenue_per_user['revenue_actual'])\n",
        "revenue_per_user=revenue_per_user.drop(['revenue_pred','revenue_actual'],axis=1)\n",
        "\n",
        "\n",
        "################NEW BOX#################################################\n",
        "\n",
        "###TESTING NEW WAY\n",
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(revenue_per_user['log_sum_pred'], revenue_per_user['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('CV RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr49MklZ7qSz"
      },
      "outputs": [],
      "source": [
        "# Create a pd.Series of features importances\n",
        "importances_grad = pd.Series(gradient.feature_importances_,\n",
        "index = X_train.columns)\n",
        "\n",
        "# Sort importances_rf\n",
        "sorted_importances_grad = importances_grad.sort_values()\n",
        "\n",
        "# Make a horizontal bar plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "sorted_importances_grad.plot(kind='barh', color='blue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9_4yiFj7qSz"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PptwFuDe7qSz"
      },
      "outputs": [],
      "source": [
        "rfr=RandomForestRegressor(max_depth=6, random_state=seed,n_estimators=500)\n",
        "rfr.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71d0Coex7qS0"
      },
      "source": [
        "**RMSE results for Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blKw7Ju17qS0"
      },
      "outputs": [],
      "source": [
        "predictions=rfr.predict(X_test)\n",
        "mse_dt = MSE(predictions, y_test)\n",
        "# Compute rmse_dt\n",
        "rmse_dt = mse_dt**(1/2)\n",
        "# Print rmse_dt\n",
        "print(\"Test set RMSE of dt: {}\".format(rmse_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaigyYkM7qS0"
      },
      "outputs": [],
      "source": [
        "# Import mean_squared_error from sklearn.metrics as MSE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Predict the labels of the training set\n",
        "y_pred_train = rfr.predict(X_train)\n",
        "\n",
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('Train RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QKyfN9j7qS0"
      },
      "source": [
        "**RMSE after aggregating into user level**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5krCgG97qS4"
      },
      "outputs": [],
      "source": [
        "y_pred_train[y_pred_train<0]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1xEPk8G7qS4"
      },
      "outputs": [],
      "source": [
        "revenue_per_user=pd.DataFrame({'user_id': train_id, 'revenue_pred': np.expm1(y_pred_train), 'revenue_actual': np.expm1(y_train)})\n",
        "revenue_per_user=revenue_per_user.groupby(['user_id']).agg({'revenue_pred':'sum','revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user['log_sum_pred']=np.log1p(revenue_per_user['revenue_pred'])\n",
        "revenue_per_user['log_sum_actual']=np.log1p(revenue_per_user['revenue_actual'])\n",
        "revenue_per_user=revenue_per_user.drop(['revenue_pred','revenue_actual'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBbu037f7qS4"
      },
      "outputs": [],
      "source": [
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(revenue_per_user['log_sum_pred'], revenue_per_user['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('Train RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V6qIX_W7qS4"
      },
      "outputs": [],
      "source": [
        "predictions[predictions<0]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hgrx1r07qS4"
      },
      "outputs": [],
      "source": [
        "revenue_per_user_test=pd.DataFrame({'user_id': test_id, 'revenue_pred': np.expm1(predictions), 'revenue_actual': np.expm1(y_test)})\n",
        "revenue_per_user_test=revenue_per_user_test.groupby(['user_id']).agg({'revenue_pred':'sum','revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user_test['log_sum_pred']=np.log1p(revenue_per_user_test['revenue_pred'])\n",
        "revenue_per_user_test['log_sum_actual']=np.log1p(revenue_per_user_test['revenue_actual'])\n",
        "revenue_per_user_test=revenue_per_user_test.drop(['revenue_pred','revenue_actual'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HA1cIYc7qS4"
      },
      "outputs": [],
      "source": [
        "mse_dt = (MSE(revenue_per_user_test['log_sum_pred'], revenue_per_user_test['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print rmse_dt\n",
        "print(\"Test set RMSE of dt: {}\".format(rmse_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1hMUxXB7qS4"
      },
      "outputs": [],
      "source": [
        "###Cross validation for aggregate results\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Compute the array containing the 10-folds CV MSEs\n",
        "\n",
        "\n",
        "cross_val_pred = cross_val_predict(rfr, X_train, y_train, cv=10,\n",
        "                                  groups = train_id,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "#RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "#print('CV RMSE: {}'.format(RMSE_CV))\n",
        "\n",
        "\n",
        "cross_val_pred[cross_val_pred<0] = 0\n",
        "\n",
        "\n",
        "revenue_per_user=pd.DataFrame({'user_id': train_id, 'revenue_pred': np.expm1(cross_val_pred),\n",
        "                               'revenue_actual': np.expm1(y_train)})\n",
        "revenue_per_user=revenue_per_user.groupby(['user_id']).agg({'revenue_pred':'sum',\n",
        "                                                            'revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user['log_sum_pred']=np.log1p(revenue_per_user['revenue_pred'])\n",
        "revenue_per_user['log_sum_actual']=np.log1p(revenue_per_user['revenue_actual'])\n",
        "revenue_per_user=revenue_per_user.drop(['revenue_pred','revenue_actual'],axis=1)\n",
        "\n",
        "\n",
        "################NEW BOX#################################################\n",
        "\n",
        "###TESTING NEW WAY\n",
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(revenue_per_user['log_sum_pred'], revenue_per_user['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('CV RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ3zbOXt7qS4"
      },
      "source": [
        "**Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsB25Yyx7qS4"
      },
      "outputs": [],
      "source": [
        "tree = DecisionTreeRegressor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9iTsBhK7qS4"
      },
      "outputs": [],
      "source": [
        "# Instantiate\n",
        "\n",
        "# Fit dt to the training set\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Compute y_pred\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Evaluate the test set RMSE\n",
        "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
        "rmse_train = MSE(y_train, y_pred_train)**(1/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEAsmTNG7qS4"
      },
      "outputs": [],
      "source": [
        "print(rmse_test)\n",
        "print(rmse_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vS9eVi17qS5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "# Compute the array containing the 10-folds CV MSEs\n",
        "MSE_CV_scores = - cross_val_score(tree, X_train, y_train, cv=5,\n",
        "                                  scoring='neg_mean_squared_error',\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "print('CV RMSE: {}'.format(RMSE_CV))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yNxC9L_7qS5"
      },
      "outputs": [],
      "source": [
        "# Create a pd.Series of features importances\n",
        "importances_tree = pd.Series(tree.feature_importances_,\n",
        "index = X_train.columns)\n",
        "\n",
        "# Sort importances_rf\n",
        "sorted_importances_tree = importances_tree.sort_values()\n",
        "\n",
        "# Make a horizontal bar plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sorted_importances_tree.plot(kind='barh', color='blue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFnxucY17qS5"
      },
      "source": [
        "**LightGBM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOI44RPg7qS5"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "lgb = LGBMRegressor(random_state=0)\n",
        "regressors = [('LightGBM', lgb)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12NgCunx7qS5"
      },
      "outputs": [],
      "source": [
        "for clf_name, clf in regressors:\n",
        "    clf.fit(X_train, y_train.ravel())\n",
        "    predictions = clf.predict(X_test)\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
        "    print('{} Train RMSE: {:.4f}'.format(clf_name, RMSE_train))\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "    RMSE_test = (MSE(y_test, y_pred_test))**(1/2)\n",
        "    print(\"{} Test RMSE: {:.4f}\".format(clf_name, RMSE_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpP9gbnO7qS5"
      },
      "outputs": [],
      "source": [
        "# cross validation\n",
        "for clf_name, clf in regressors:\n",
        "    MSE_CV_scores = - cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "    print('{} CV RMSE: {:.4f}'.format(clf_name, RMSE_CV))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQiYN0mu7qS5"
      },
      "source": [
        "**RMSE after aggregating into user level**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHlWDFza7qS5"
      },
      "outputs": [],
      "source": [
        "y_pred_train[y_pred_train<0]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MB77Xy67qS5"
      },
      "outputs": [],
      "source": [
        "revenue_per_user=pd.DataFrame({'user_id': train_id, 'revenue_pred': np.expm1(y_pred_train), 'revenue_actual': np.expm1(y_train)})\n",
        "revenue_per_user=revenue_per_user.groupby(['user_id']).agg({'revenue_pred':'sum','revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user['log_sum_pred']=np.log1p(revenue_per_user['revenue_pred'])\n",
        "revenue_per_user['log_sum_actual']=np.log1p(revenue_per_user['revenue_actual'])\n",
        "revenue_per_user=revenue_per_user.drop(['revenue_pred','revenue_actual'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvWfV87m7qS5"
      },
      "outputs": [],
      "source": [
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(revenue_per_user['log_sum_pred'], revenue_per_user['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('Train RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO2QLESi7qS5"
      },
      "outputs": [],
      "source": [
        "predictions[predictions<0]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82SI_9_q7qS5"
      },
      "outputs": [],
      "source": [
        "revenue_per_user_test=pd.DataFrame({'user_id': test_id, 'revenue_pred': np.expm1(predictions), 'revenue_actual': np.expm1(y_test)})\n",
        "revenue_per_user_test=revenue_per_user_test.groupby(['user_id']).agg({'revenue_pred':'sum','revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user_test['log_sum_pred']=np.log1p(revenue_per_user_test['revenue_pred'])\n",
        "revenue_per_user_test['log_sum_actual']=np.log1p(revenue_per_user_test['revenue_actual'])\n",
        "revenue_per_user_test=revenue_per_user_test.drop(['revenue_pred','revenue_actual'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUX3CiCs7qS5"
      },
      "outputs": [],
      "source": [
        "rmse_dt = (MSE(revenue_per_user_test['log_sum_pred'], revenue_per_user_test['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print rmse_dt\n",
        "print(\"Test set RMSE of dt: {}\".format(rmse_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAmM0xEN7qS5"
      },
      "outputs": [],
      "source": [
        "###Cross validation for aggregate results\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Compute the array containing the 10-folds CV MSEs\n",
        "\n",
        "\n",
        "cross_val_pred = cross_val_predict(lgb, X_train, y_train, cv=10,\n",
        "                                  groups = train_id,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "#RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "#print('CV RMSE: {}'.format(RMSE_CV))\n",
        "\n",
        "\n",
        "cross_val_pred[cross_val_pred<0] = 0\n",
        "\n",
        "\n",
        "revenue_per_user=pd.DataFrame({'user_id': train_id, 'revenue_pred': np.expm1(cross_val_pred),\n",
        "                               'revenue_actual': np.expm1(y_train)})\n",
        "revenue_per_user=revenue_per_user.groupby(['user_id']).agg({'revenue_pred':'sum',\n",
        "                                                            'revenue_actual':'sum'}).reset_index()\n",
        "revenue_per_user['log_sum_pred']=np.log1p(revenue_per_user['revenue_pred'])\n",
        "revenue_per_user['log_sum_actual']=np.log1p(revenue_per_user['revenue_actual'])\n",
        "revenue_per_user=revenue_per_user.drop(['revenue_pred','revenue_actual'],axis=1)\n",
        "\n",
        "\n",
        "################NEW BOX#################################################\n",
        "\n",
        "###TESTING NEW WAY\n",
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(revenue_per_user['log_sum_pred'], revenue_per_user['log_sum_actual']))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('CV RMSE: {}'.format(RMSE_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p3QlU-h7qS5"
      },
      "source": [
        "### Modeling on user Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmPlKymO7qS5"
      },
      "outputs": [],
      "source": [
        "# load required library\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Importing Classifier Modules\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# cross validation\n",
        "import sklearn\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYriTQ7g7qS6"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_pickle('customer_dataset.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPDoJ93B7qS6"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO8iJ3w_7qS6"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zpjKJhT7qS6"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['id','logRevenue'], axis=1).values\n",
        "y = df['logRevenue'].values\n",
        "\n",
        "# Split data into 75% train and 25% test\n",
        "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.25, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QOuN8887qS6"
      },
      "outputs": [],
      "source": [
        "lr = LinearRegression()\n",
        "dt = DecisionTreeRegressor()\n",
        "gb = GradientBoostingRegressor()\n",
        "kn = KNeighborsRegressor()\n",
        "rf = RandomForestRegressor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO1hw70v7qS6"
      },
      "outputs": [],
      "source": [
        "regressors = [('Linear Regression', lr),\n",
        "              ('Decision Tree Regressor', dt),\n",
        "              ('Gradient Boosting Regressor', gb),\n",
        "              ('KNeighbors Regressor', kn),\n",
        "              ('Random Forest Regressor', rf)\n",
        "             ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOzYl6wr7qS6"
      },
      "outputs": [],
      "source": [
        "for clf_name, clf in regressors:\n",
        "    clf.fit(X_train, y_train.ravel())\n",
        "    predicted = clf.predict(X_test)\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
        "    print('{} Train RMSE: {:.4f}'.format(clf_name, RMSE_train))\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "    RMSE_test = (MSE(y_test, y_pred_test))**(1/2)\n",
        "    print(\"{} Test RMSE: {:.4f}\".format(clf_name, RMSE_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "DUsJQVjl7qS6"
      },
      "outputs": [],
      "source": [
        "# cross validation\n",
        "for clf_name, clf in regressors:\n",
        "    MSE_CV_scores = - cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "    print('{} CV RMSE: {:.4f}'.format(clf_name, RMSE_CV))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB8LIqJ77qS6"
      },
      "source": [
        "### Light GBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNU01L0_7qS6"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myYZ7NRl7qS6"
      },
      "outputs": [],
      "source": [
        "lgb = LGBMRegressor(random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gsA0Hdq7qS6"
      },
      "outputs": [],
      "source": [
        "regressors = [('LightGBM', lgb)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5PstRGw7qS6"
      },
      "outputs": [],
      "source": [
        "for clf_name, clf in regressors:\n",
        "    clf.fit(X_train, y_train.ravel())\n",
        "    predicted = clf.predict(X_test)\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
        "    print('{} Train RMSE: {:.4f}'.format(clf_name, RMSE_train))\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "    RMSE_test = (MSE(y_test, y_pred_test))**(1/2)\n",
        "    print(\"{} Test RMSE: {:.4f}\".format(clf_name, RMSE_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td0XXVOE7qS6"
      },
      "outputs": [],
      "source": [
        "# cross validation\n",
        "for clf_name, clf in regressors:\n",
        "    MSE_CV_scores = - cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "    print('{} CV RMSE: {:.4f}'.format(clf_name, RMSE_CV))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rZW1m-M7qS6"
      },
      "outputs": [],
      "source": [
        "# Tune Paramaters with lightgbm\n",
        "\n",
        "# Create parameters to search\n",
        "gridParams = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2], # default 0.1\n",
        "    'n_estimators': [40, 100, 150], # default 100\n",
        "    'num_leaves': [21,31,41], # default 31\n",
        "    'random_state': [0], # Updated from 'seed'\n",
        "    'max_depth': [-1, 20],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C05ddjs27qS6"
      },
      "outputs": [],
      "source": [
        "lgb_grid = LGBMRegressor(boosting_type= 'gbdt',\n",
        "          objective = 'regression',\n",
        "          n_jobs = 3, # Updated from 'nthread'\n",
        "          max_depth = 1000)\n",
        "\n",
        "# Create the grid\n",
        "grid = GridSearchCV(lgb_grid, gridParams, cv=5)\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93pvsCzu7qS6"
      },
      "outputs": [],
      "source": [
        "print('Best parameters found by grid search are:', grid.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcCxLu2H7qS6"
      },
      "outputs": [],
      "source": [
        "y_pred_train = grid.predict(X_train)\n",
        "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
        "print('LightGBM Tuned Train RMSE: {:.4f}'.format(RMSE_train))\n",
        "y_pred_test = grid.predict(X_test)\n",
        "RMSE_test = (MSE(y_test, y_pred_test))**(1/2)\n",
        "print(\"LightGBM Tuned RMSE: {:.4f}\".format(RMSE_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2XmW4Vl7qS7"
      },
      "outputs": [],
      "source": [
        "# Tuned Set cross validation\n",
        "lgbm_tuned = LGBMRegressor(learning_rate= 0.05, max_depth= -1, n_estimators=150, num_leaves= 31, random_state=0)\n",
        "regressors = [('LightGBM Tuned', lgbm_tuned)]\n",
        "for clf_name, clf in regressors:\n",
        "    MSE_CV_scores = - cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "    print('{} CV RMSE: {:.4f}'.format(clf_name, RMSE_CV))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv6nJWlr7qS7"
      },
      "source": [
        "### Ridge Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-k9dief7qS7"
      },
      "outputs": [],
      "source": [
        "ridge = RidgeCV(cv=10)\n",
        "\n",
        "# fit or train the linear regression model on the training set and store␣\n",
        "ridge.fit(X_train, y_train)\n",
        "# show the alpha parameter used in final ridgeCV model ridge.alpha_\n",
        "ridge.alpha_\n",
        "\n",
        "#\n",
        "train_pred = ridge.predict(X_train)\n",
        "ridge_test_pred = ridge.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmeReZ_77qS7"
      },
      "outputs": [],
      "source": [
        "#print RMSE of training predictions\n",
        "print('RMSE on train for regression: ', np.sqrt(MSE(y_train, train_pred)))\n",
        "print('RMSE on test for regression: ', np.sqrt(MSE(y_test,ridge_test_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxeUkoIm7qS7"
      },
      "outputs": [],
      "source": [
        "# Compute the array containing the 10-folds CV MSEs\n",
        "MSE_CV_scores = - cross_val_score(ridge, X_train, y_train, cv=10,\n",
        "                                  scoring='neg_mean_squared_error',\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "print('CV RMSE: {}'.format(RMSE_CV))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PITz4f6L7qS7"
      },
      "source": [
        "### Gradient Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lBE8sjk7qS7"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn import ensemble\n",
        "# Fit regression model\n",
        "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
        "          'learning_rate': 0.01, 'loss': 'ls'}\n",
        "gradient = ensemble.GradientBoostingRegressor(**params)\n",
        "\n",
        "gradient.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2646b8Z7qS7"
      },
      "outputs": [],
      "source": [
        "# Predict the test set labels 'y_pred'\n",
        "y_pred = gradient.predict(X_test)\n",
        "y_pred_train=gradient.predict(X_train)\n",
        "\n",
        "# Evaluate the test set RMSE\n",
        "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
        "rmse_train = MSE(y_train, y_pred_train)**(1/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euSjLX167qS7"
      },
      "outputs": [],
      "source": [
        "# Create a pd.Series of features importances\n",
        "importances_grad = pd.Series(gradient.feature_importances_,\n",
        "index = X_train.columns)\n",
        "\n",
        "# Sort importances_rf\n",
        "sorted_importances_grad = importances_grad.sort_values()\n",
        "\n",
        "# Make a horizontal bar plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sorted_importances_grad.plot(kind='barh', color='blue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxQAPp_u7qS7"
      },
      "outputs": [],
      "source": [
        "# Print the test set RMSE\n",
        "print('Train set RMSE for gradient on train data: {:.3f}'.format(rmse_train))\n",
        "print('Test set RMSE for gradient on test data: {:.3f}'.format(rmse_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4iO_yGC7qS7"
      },
      "outputs": [],
      "source": [
        "# Compute the array containing the 10-folds CV MSEs\n",
        "MSE_CV_scores = - cross_val_score(gradient, X_train, y_train, cv=10,\n",
        "                                  scoring='neg_mean_squared_error',\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "print('CV RMSE: {}'.format(RMSE_CV))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9uPJu9l7qS7"
      },
      "source": [
        "### XGBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evt8W2gV7qS7"
      },
      "outputs": [],
      "source": [
        "# xgb models only accept 'DMatrix' input; convert the data here\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "# parameter grid for xgb model\n",
        "param_dict = { 'max_depth':6, 'min_child_weight': 1,\n",
        "              'eta':.1, 'subsample': 1,\n",
        "              'colsample_bytree': 1, # Other parameters 'objective':'reg:linear', 'eval_metric':'rmse'\n",
        "}\n",
        "# train XGB model on split training data using split test data\n",
        "num_boost_round=999\n",
        "model_xgb = xgb.train( param_dict, dtrain,\n",
        "                  num_boost_round=num_boost_round,\n",
        "                  evals=[(dtest, \"Test\")],\n",
        "                  early_stopping_rounds=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT-2nK5h7qS8"
      },
      "outputs": [],
      "source": [
        "dpred=xgb.DMatrix(X_train)\n",
        "\n",
        "# predict RMSE of model on training data\n",
        "print(\"RMSE of XGB model on train data: \", np.sqrt(MSE(model_xgb.predict(dpred), y_train)))\n",
        "# predict RMSE of model on test data\n",
        "model_xgb_test_pred = model_xgb.predict(xgb.DMatrix(X_test))\n",
        "print(\"RMSE of XGB model on test data: \", np.sqrt(MSE(model_xgb_test_pred, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "YDdEpDJa7qS8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "plt.show(plot_importance(model_xgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUHIwykv7qS8"
      },
      "source": [
        "## Customer Segmentation and LTV Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfRZIDld7qS8"
      },
      "source": [
        "In this section, we try to do customer segmentation with RFM (Recency, Frequency and Monetary Value) scores. Since we have the data for the whole year of 2017, we seperate it into two sets: January-September and Octobor-December. Then we will do customer segmentation for the first 9 months and predict the customer life time value (LTV) for the last 3 months.\n",
        "\n",
        "**Customer Segmentation (First 9 Months)** <br>\n",
        "We calculate the recency (how many days past since the last transaction), frequency (how many transactions within 9 months) and log revenue for all transactions. Then we do clustering for these 3 features so that we have 3 clustering features. We re-order the cluster order the make the biggest cluster number represents the best results. Thus, we calculate the overall score by summing up these 3 scores. Then we segment the customer into 4 groups according to the overall scores.\n",
        "\n",
        "**LTV Prediction (Last 3 Months)**  <br>\n",
        "First, we use the log revenue of the last 3 months to do a LTV clustering. Then we use the 9-month features to predict the 3-month LTV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chJz_NGH7qS8"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from datetime import datetime, timedelta,date\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import chart_studio.plotly as py\n",
        "import plotly.offline as pyoff\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "u07GteuA7qS8"
      },
      "outputs": [],
      "source": [
        "pyoff.init_notebook_mode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elj0z3eu7qS8"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "df = pd.read_pickle('2017_clean.pkl')\n",
        "train = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6uQCI9q7qS8"
      },
      "outputs": [],
      "source": [
        "# keep related information and change data type\n",
        "train = train[['fullVisitorId', 'date', 'totals.totalTransactionRevenue']]\n",
        "train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%Y%m%d\") # seting the column as pandas datetime\n",
        "train['totals.totalTransactionRevenue']=train['totals.totalTransactionRevenue'].astype('float')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPmOr72A7qS8"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLRcrBPd7qS8"
      },
      "outputs": [],
      "source": [
        "train['date'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FpPCrq37qS8"
      },
      "outputs": [],
      "source": [
        "# select data from Jan to Sept\n",
        "train_9 = train[(train.date < date(2017,10,1)) & (train.date >= date(2017,1,1))].reset_index(drop=True)\n",
        "# select data from Oct to Dec\n",
        "train_12 = train[(train.date >= date(2017,10,1)) & (train.date <= date(2017,12,31))].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "IbVWUTJO7qS8"
      },
      "outputs": [],
      "source": [
        "train_9['date'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rfUQtoQ7qS8"
      },
      "outputs": [],
      "source": [
        "train_12['date'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiO9ZnKy7qS8"
      },
      "outputs": [],
      "source": [
        "customer = pd.DataFrame(train['fullVisitorId'].unique())\n",
        "customer.columns = ['fullVisitorId']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOgVLwAv7qS8"
      },
      "source": [
        "### Recency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGlpNJCZ7qS9"
      },
      "outputs": [],
      "source": [
        "max_purchase = train_9.groupby('fullVisitorId').date.max().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N03LTk227qS9"
      },
      "outputs": [],
      "source": [
        "max_purchase.columns = ['fullVisitorId','MaxPurchaseDate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L2u7Vkd7qS9"
      },
      "outputs": [],
      "source": [
        "max_purchase['Recency'] = (max_purchase['MaxPurchaseDate'].max() - max_purchase['MaxPurchaseDate']).dt.days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udypCo647qS9"
      },
      "outputs": [],
      "source": [
        "user_recency = pd.merge(customer, max_purchase[['fullVisitorId','Recency']], on='fullVisitorId')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gtbd0Ut7qS9"
      },
      "outputs": [],
      "source": [
        "user_recency.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1Aol0_Q7qS9"
      },
      "outputs": [],
      "source": [
        "user_recency.Recency.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hewATHgn7qS9"
      },
      "outputs": [],
      "source": [
        "plot_data = [\n",
        "    go.Histogram(\n",
        "        x=user_recency['Recency']\n",
        "    )\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        title='Recency'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDhCIJLH7qS9"
      },
      "outputs": [],
      "source": [
        "# Elbow Method\n",
        "sse={}\n",
        "recency = user_recency[['Recency']]\n",
        "for k in range(1, 10):\n",
        "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(recency)\n",
        "    recency[\"clusters\"] = kmeans.labels_\n",
        "    sse[k] = kmeans.inertia_\n",
        "plt.figure()\n",
        "plt.plot(list(sse.keys()), list(sse.values()))\n",
        "plt.xlabel(\"Number of cluster\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQLBgPtM7qS9"
      },
      "outputs": [],
      "source": [
        "# let's try 4 first\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(user_recency[['Recency']])\n",
        "user_recency['RecencyCluster'] = kmeans.predict(user_recency[['Recency']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yKrRQH227qTK"
      },
      "outputs": [],
      "source": [
        "user_recency.groupby('RecencyCluster')['Recency'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hON3MxTT7qTK"
      },
      "outputs": [],
      "source": [
        "def order_cluster(cluster_field_name, target_field_name,df,ascending):\n",
        "    new_cluster_field_name = 'new_' + cluster_field_name\n",
        "    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n",
        "    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n",
        "    df_new['index'] = df_new.index\n",
        "    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n",
        "    df_final = df_final.drop([cluster_field_name],axis=1)\n",
        "    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n",
        "    return df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC6O7PGd7qTK"
      },
      "outputs": [],
      "source": [
        "# call the funtion above to order cluster\n",
        "user_recency = order_cluster('RecencyCluster', 'Recency', user_recency,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "IonnaYwg7qTK"
      },
      "outputs": [],
      "source": [
        "user_recency.groupby('RecencyCluster')['Recency'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZVqxYT0N7qTK"
      },
      "outputs": [],
      "source": [
        "user_recency.groupby('RecencyCluster')['Recency'].agg(['count','mean']).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8BoK8zf7qTK"
      },
      "outputs": [],
      "source": [
        "user_recency.groupby('RecencyCluster')['Recency'].hist()\n",
        "plt.xlabel('Recency')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Recency_Cluster')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VAj7S8L7qTK"
      },
      "source": [
        "### Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-wdObc37qTK"
      },
      "outputs": [],
      "source": [
        "user_frequency = train_9.groupby('fullVisitorId').date.count().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE_uM0vM7qTK"
      },
      "outputs": [],
      "source": [
        "user_frequency.columns = ['fullVisitorId','Frequency']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "YaYag95L7qTK"
      },
      "outputs": [],
      "source": [
        "user_frequency.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4onNSF77qTK"
      },
      "outputs": [],
      "source": [
        "user = pd.merge(user_recency, user_frequency, on='fullVisitorId')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fFLNDaL7qTL"
      },
      "outputs": [],
      "source": [
        "user.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBq77s_K7qTL"
      },
      "outputs": [],
      "source": [
        "user.Frequency.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOXhL2WR7qTL"
      },
      "outputs": [],
      "source": [
        "plot_data = [\n",
        "    go.Histogram(\n",
        "        x=user['Frequency']\n",
        "    )\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        title='Frequency'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qz4CD0a7qTL"
      },
      "outputs": [],
      "source": [
        "# Elbow Method\n",
        "sse={}\n",
        "frequency = user[['Frequency']]\n",
        "for k in range(1, 15):\n",
        "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(frequency)\n",
        "    frequency[\"clusters\"] = kmeans.labels_\n",
        "    sse[k] = kmeans.inertia_\n",
        "plt.figure()\n",
        "plt.plot(list(sse.keys()), list(sse.values()))\n",
        "plt.xlabel(\"Number of cluster\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsWn4JeT7qTL"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(user[['Frequency']])\n",
        "user['FrequencyCluster'] = kmeans.predict(user[['Frequency']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOJeMtRo7qTL"
      },
      "outputs": [],
      "source": [
        "user = order_cluster('FrequencyCluster', 'Frequency',user,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "7OK97UmH7qTL"
      },
      "outputs": [],
      "source": [
        "user.groupby('FrequencyCluster')['Frequency'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ubgi2JIK7qTL"
      },
      "outputs": [],
      "source": [
        "user.groupby('FrequencyCluster')['Frequency'].agg(['count','mean']).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wANGWZRM7qTL"
      },
      "source": [
        "### Monetary Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMs1OowX7qTL"
      },
      "outputs": [],
      "source": [
        "revenue = train_9.groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JFii0z5n7qTL"
      },
      "outputs": [],
      "source": [
        "revenue.columns = ['fullVisitorId', 'Revenue']\n",
        "revenue['logRevenue'] = np.log(1+revenue['Revenue'])\n",
        "revenue = revenue.drop(['Revenue'], axis=1)\n",
        "revenue.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtVxZNm77qTL"
      },
      "outputs": [],
      "source": [
        "user = pd.merge(user, revenue, on='fullVisitorId')\n",
        "user.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGL3AyH07qTL"
      },
      "outputs": [],
      "source": [
        "user.logRevenue.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhCGdzn_7qTL"
      },
      "outputs": [],
      "source": [
        "# the elbow method show 2\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(user[['logRevenue']])\n",
        "user['logRevenueCluster'] = kmeans.predict(user[['logRevenue']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwu8b-Oa7qTL"
      },
      "outputs": [],
      "source": [
        "user = order_cluster('logRevenueCluster', 'logRevenue',user,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5ylthA3J7qTL"
      },
      "outputs": [],
      "source": [
        "user.groupby('logRevenueCluster')['logRevenue'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zcR5OqHo7qTL"
      },
      "outputs": [],
      "source": [
        "user.groupby('logRevenueCluster')['logRevenue'].agg(['count','mean']).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOs55B5f7qTL"
      },
      "source": [
        "### Overall Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "FXXVZrRA7qTL"
      },
      "outputs": [],
      "source": [
        "user.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ratPnar7qTL"
      },
      "outputs": [],
      "source": [
        "user['OverallScore'] = user['RecencyCluster'] + user['FrequencyCluster'] + user['logRevenueCluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "1UmFL0jK7qTL"
      },
      "outputs": [],
      "source": [
        "user.groupby('OverallScore')['Recency','Frequency','logRevenue'].mean().reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89TWOtV27qTL"
      },
      "source": [
        "Looks like overall score 7 is the highest value customer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU9hr1H-7qTM"
      },
      "outputs": [],
      "source": [
        "user['Segment'] = 'Low-Value'\n",
        "user.loc[user['OverallScore']>0,'Segment'] = 'Mid1-Value'\n",
        "user.loc[user['OverallScore']>3,'Segment'] = 'Mid2-Value'\n",
        "user.loc[user['OverallScore']>5,'Segment'] = 'High-Value'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUmphtoI7qTM"
      },
      "outputs": [],
      "source": [
        "sg = user.groupby('Segment')['logRevenue'].agg(['count','mean']).sort_values(by=['mean']).reset_index()\n",
        "sg.columns = ['Segment','count','mean_logRevenue_9']\n",
        "sg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF1PRfEV7qTM"
      },
      "outputs": [],
      "source": [
        "user_sample = user.sample(20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qizCBlB07qTM"
      },
      "outputs": [],
      "source": [
        "tx_graph = user_sample\n",
        "\n",
        "plot_data = [\n",
        "    go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Low-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'Low-Value'\")['Frequency'],\n",
        "        mode='markers',\n",
        "        name='Low',\n",
        "        marker= dict(size= 7,\n",
        "            line= dict(width=1),\n",
        "            color= 'blue',\n",
        "            opacity= 0.8\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Mid1-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'Mid1-Value'\")['Frequency'],\n",
        "        mode='markers',\n",
        "        name='Mid1',\n",
        "        marker= dict(size= 9,\n",
        "            line= dict(width=1),\n",
        "            color= 'green',\n",
        "            opacity= 0.5\n",
        "           )\n",
        "    ),\n",
        "            go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Mid2-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'Mid2-Value'\")['Frequency'],\n",
        "        mode='markers',\n",
        "        name='Mid2',\n",
        "        marker= dict(size= 9,\n",
        "            line= dict(width=1),\n",
        "            color= 'orange',\n",
        "            opacity= 0.5\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'High-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'High-Value'\")['Frequency'],\n",
        "        mode='markers',\n",
        "        name='High',\n",
        "        marker= dict(size= 11,\n",
        "            line= dict(width=1),\n",
        "            color= 'red',\n",
        "            opacity= 0.9\n",
        "           )\n",
        "    ),\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        yaxis= {'title': \"Frequency\"},\n",
        "        xaxis= {'title': \"Recency\"},\n",
        "        title='Segments'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "KLOCvF9H7qTM"
      },
      "outputs": [],
      "source": [
        "tx_graph = user_sample\n",
        "\n",
        "plot_data = [\n",
        "    go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Low-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'Low-Value'\")['logRevenue'],\n",
        "        mode='markers',\n",
        "        name='Low',\n",
        "        marker= dict(size= 7,\n",
        "            line= dict(width=1),\n",
        "            color= 'blue',\n",
        "            opacity= 0.8\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Mid1-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'Mid1-Value'\")['logRevenue'],\n",
        "        mode='markers',\n",
        "        name='Mid1',\n",
        "        marker= dict(size= 9,\n",
        "            line= dict(width=1),\n",
        "            color= 'green',\n",
        "            opacity= 0.5\n",
        "           )\n",
        "    ),\n",
        "            go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Mid2-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'Mid2-Value'\")['logRevenue'],\n",
        "        mode='markers',\n",
        "        name='Mid2',\n",
        "        marker= dict(size= 9,\n",
        "            line= dict(width=1),\n",
        "            color= 'orange',\n",
        "            opacity= 0.5\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'High-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'High-Value'\")['logRevenue'],\n",
        "        mode='markers',\n",
        "        name='High',\n",
        "        marker= dict(size= 11,\n",
        "            line= dict(width=1),\n",
        "            color= 'red',\n",
        "            opacity= 0.9\n",
        "           )\n",
        "    ),\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        yaxis= {'title': \"logRevenue\"},\n",
        "        xaxis= {'title': \"Recency\"},\n",
        "        title='Segments'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Zw8zHFjt7qTM"
      },
      "outputs": [],
      "source": [
        "tx_graph = user_sample\n",
        "\n",
        "plot_data = [\n",
        "    go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Low-Value'\")['Frequency'],\n",
        "        y=tx_graph.query(\"Segment == 'Low-Value'\")['logRevenue'],\n",
        "        mode='markers',\n",
        "        name='Low',\n",
        "        marker= dict(size= 7,\n",
        "            line= dict(width=1),\n",
        "            color= 'blue',\n",
        "            opacity= 0.8\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Mid1-Value'\")['Frequency'],\n",
        "        y=tx_graph.query(\"Segment == 'Mid1-Value'\")['logRevenue'],\n",
        "        mode='markers',\n",
        "        name='Mid1',\n",
        "        marker= dict(size= 9,\n",
        "            line= dict(width=1),\n",
        "            color= 'green',\n",
        "            opacity= 0.5\n",
        "           )\n",
        "    ),\n",
        "            go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Mid2-Value'\")['Frequency'],\n",
        "        y=tx_graph.query(\"Segment == 'Mid2-Value'\")['logRevenue'],\n",
        "        mode='markers',\n",
        "        name='Mid2',\n",
        "        marker= dict(size= 9,\n",
        "            line= dict(width=1),\n",
        "            color= 'orange',\n",
        "            opacity= 0.5\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'High-Value'\")['Frequency'],\n",
        "        y=tx_graph.query(\"Segment == 'High-Value'\")['logRevenue'],\n",
        "        mode='markers',\n",
        "        name='High',\n",
        "        marker= dict(size= 11,\n",
        "            line= dict(width=1),\n",
        "            color= 'red',\n",
        "            opacity= 0.9\n",
        "           )\n",
        "    ),\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        yaxis= {'title': \"logRevenue\"},\n",
        "        xaxis= {'title': \"Frequency\"},\n",
        "        title='Segments'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT5u1dlx7qTM"
      },
      "source": [
        "### Life Time Value Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBTp_jTl7qTM"
      },
      "outputs": [],
      "source": [
        "user.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoxQ0HTj7qTM"
      },
      "outputs": [],
      "source": [
        "train_12.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn7Eo9YH7qTM"
      },
      "outputs": [],
      "source": [
        "revenue_12 = train_12.groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
        "revenue_12.columns = ['fullVisitorId','Revenue_12']\n",
        "revenue_12['logRevenue_12'] = np.log(1+revenue_12['Revenue_12'])\n",
        "revenue_12 = revenue_12.drop(['Revenue_12'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUCaEKLP7qTM"
      },
      "outputs": [],
      "source": [
        "revenue_12.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za9f3bk17qTM"
      },
      "outputs": [],
      "source": [
        "# merge 9-month features with 3-month revenue\n",
        "merge = pd.merge(user, revenue_12, on='fullVisitorId', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPiN1AZA7qTM"
      },
      "outputs": [],
      "source": [
        "merge.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zWRDMBI7qTM"
      },
      "outputs": [],
      "source": [
        "merge = merge.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_3rjTb5Q7qTM"
      },
      "outputs": [],
      "source": [
        "sg_12 = merge.groupby('Segment')['logRevenue_12'].agg(['count','mean']).sort_values(by=['mean']).reset_index()\n",
        "sg_12.columns = ['Segment','count','mean_logRevenue_12']\n",
        "sg_12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgzVlqGp7qTM"
      },
      "outputs": [],
      "source": [
        "merge.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7D6Jq2H7qTM"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(merge[['logRevenue_12']])\n",
        "merge['LTVCluster'] = kmeans.predict(merge[['logRevenue_12']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZibbGKel7qTM"
      },
      "outputs": [],
      "source": [
        "merge = order_cluster('LTVCluster', 'logRevenue_12',merge,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "oD7rcnRf7qTM"
      },
      "outputs": [],
      "source": [
        "merge.groupby('LTVCluster')['logRevenue_12'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttvYteGH7qTM"
      },
      "outputs": [],
      "source": [
        "ltv = merge.groupby('LTVCluster')['logRevenue_12'].agg(['count','mean']).reset_index()\n",
        "ltv.columns = ['LTVCluster','count','mean_logRevenue_12']\n",
        "ltv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqEeli1C7qTM"
      },
      "outputs": [],
      "source": [
        "cluster = merge.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQmCx1uW7qTN"
      },
      "outputs": [],
      "source": [
        "cluster.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIg1WNFH7qTN"
      },
      "outputs": [],
      "source": [
        "# dummy Segment\n",
        "cluster['Segment_High']=0\n",
        "cluster.loc[cluster['Segment']=='High-Value','Segment_High']=1\n",
        "cluster['Segment_Mid1']=0\n",
        "cluster.loc[cluster['Segment']=='Mid1-Value','Segment_Mid1']=1\n",
        "cluster['Segment_Mid2']=0\n",
        "cluster.loc[cluster['Segment']=='Mid2-Value','Segment_Mid2']=1\n",
        "cluster['Segment_Low']=0\n",
        "cluster.loc[cluster['Segment']=='Low-Value','Segment_Low']=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nFhf0x67qTN"
      },
      "outputs": [],
      "source": [
        "cluster = cluster.drop(['Segment'],axis=1)\n",
        "cluster.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "3nvl70EV7qTN"
      },
      "outputs": [],
      "source": [
        "# corelation with 3-month (Oct-Dec) LTVCluster\n",
        "corr_matrix = cluster.corr()\n",
        "corr_matrix['LTVCluster'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H_m8K1Q7qTN"
      },
      "source": [
        "### Building Model (lightGBM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVhZ3spl7qTN"
      },
      "outputs": [],
      "source": [
        "X = cluster.drop(['LTVCluster','logRevenue_12','fullVisitorId'],axis=1)\n",
        "y = cluster['LTVCluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QJLKC9N7qTN"
      },
      "outputs": [],
      "source": [
        "X_columns = cluster.drop(['LTVCluster','logRevenue_12','fullVisitorId'],axis=1).columns\n",
        "y_column = ['LTVCluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgx8PqFW7qTN"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obucTtLg7qTN"
      },
      "outputs": [],
      "source": [
        "# using SMOTE for unbalanced data\n",
        "X, y= SMOTE().fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bszaKNbP7qTN"
      },
      "outputs": [],
      "source": [
        "# change X, y to dataframe so that we can keep column names\n",
        "tX = pd.DataFrame(X, columns=X_columns)\n",
        "ty = pd.DataFrame(y, columns=y_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kbnf4ci7qTN"
      },
      "outputs": [],
      "source": [
        "# take a look at y after SMOTE, 4 clusters have the same samples now\n",
        "ty.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMoWBvN67qTN"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tX, ty, test_size=0.2, random_state=56)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwIUpAH57qTN"
      },
      "outputs": [],
      "source": [
        "# fitting lightGBM\n",
        "params = {\"objective\" : \"multiclass\",\n",
        "          \"num_class\": 4,\n",
        "          \"metric\" : \"multi_error\",\n",
        "          \"num_leaves\" : 30,\n",
        "          \"min_child_weight\" : 50,\n",
        "          \"learning_rate\" : 0.1,\n",
        "          \"bagging_fraction\" : 0.7,\n",
        "          \"feature_fraction\" : 0.7,\n",
        "          \"reg_alpha\": 0.15,\n",
        "          \"reg_lambda\": 0.15,\n",
        "          \"min_child_weight\": 50,\n",
        "          \"bagging_seed\" : 420,\n",
        "          \"verbosity\" : -1\n",
        "         }\n",
        "lg_train = lgb.Dataset(X_train, label=y_train)\n",
        "lg_test = lgb.Dataset(X_test, label=y_test)\n",
        "model = lgb.train(params, lg_train, 1000, valid_sets=[lg_test], early_stopping_rounds=50, verbose_eval=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpi3LWXG7qTN"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhrOO_qa7qTN"
      },
      "outputs": [],
      "source": [
        "y_pred[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyIUxpXp7qTN"
      },
      "outputs": [],
      "source": [
        "# change y_pred format by select the index of maximum probability\n",
        "y_pred_list = [int(np.where(i == np.amax(i))[0]) for i in y_pred]\n",
        "y_pred_ar = np.array(y_pred_list)\n",
        "y_pred_ar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFC4SiHo7qTN"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred_ar))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qauL-yP7qTN"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "lgb.plot_importance(model, height=0.8, ax=ax)\n",
        "ax.grid(False)\n",
        "plt.ylabel('Feature', size=12)\n",
        "plt.xlabel('Importance', size=12)\n",
        "plt.title(\"Importance of the Features of our LightGBM Model\", fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5spp4MF7qTN"
      },
      "source": [
        "### Adding Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL9dNwG57qTN"
      },
      "source": [
        "#### 1. Continent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed0olXcZ7qTN"
      },
      "outputs": [],
      "source": [
        "ft = df.copy()\n",
        "ft[\"date\"] = pd.to_datetime(ft[\"date\"], format=\"%Y%m%d\")\n",
        "# select data from Jan to Sept\n",
        "ft = ft[(ft.date < date(2017,10,1)) & (ft.date >= date(2017,1,1))].reset_index(drop=True)\n",
        "ft.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPv6VUN97qTN"
      },
      "outputs": [],
      "source": [
        "ft1 = ft[['fullVisitorId','geoNetwork.continent']]\n",
        "ft1 = ft1.join(pd.get_dummies(ft1['geoNetwork.continent'])).drop(['(not set)'],axis = 1).drop(['geoNetwork.continent'], axis=1)\n",
        "\n",
        "ft1 = ft1.groupby(['fullVisitorId']).agg(['max'])\n",
        "ft1.columns = ['_'.join(col).strip() for col in ft1.columns.values]\n",
        "ft1 = ft1.reset_index()\n",
        "\n",
        "ft1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "UEL1q5AV7qTN"
      },
      "outputs": [],
      "source": [
        "# merge continent features with cluster\n",
        "ft1_df = pd.merge(cluster, ft1, on='fullVisitorId')\n",
        "ft1_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxyvQFZa7qTO"
      },
      "outputs": [],
      "source": [
        "X = ft1_df.drop(['LTVCluster','logRevenue_12','fullVisitorId'],axis=1)\n",
        "y = ft1_df['LTVCluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDPzsIJo7qTO"
      },
      "outputs": [],
      "source": [
        "X_columns = ft1_df.drop(['LTVCluster','logRevenue_12','fullVisitorId'],axis=1).columns\n",
        "y_column = ['LTVCluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC0KI8Cf7qTO"
      },
      "outputs": [],
      "source": [
        "# using SMOTE for unbalanced data\n",
        "X, y= SMOTE().fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2zEBIRp7qTO"
      },
      "outputs": [],
      "source": [
        "# change X, y to dataframe so that we can keep column names\n",
        "tX = pd.DataFrame(X, columns=X_columns)\n",
        "ty = pd.DataFrame(y, columns=y_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3SBuaQN7qTO"
      },
      "outputs": [],
      "source": [
        "# take a look at y after SMOTE, 4 clusters have the same samples now\n",
        "ty.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHUTMZY67qTO"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tX, ty, test_size=0.2, random_state=56)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNkpUsah7qTO"
      },
      "outputs": [],
      "source": [
        "# fitting lightGBM\n",
        "params = {\"objective\" : \"multiclass\",\n",
        "          \"num_class\": 4,\n",
        "          \"metric\" : \"multi_error\",\n",
        "          \"num_leaves\" : 30,\n",
        "          \"min_child_weight\" : 50,\n",
        "          \"learning_rate\" : 0.1,\n",
        "          \"bagging_fraction\" : 0.7,\n",
        "          \"feature_fraction\" : 0.7,\n",
        "          \"reg_alpha\": 0.15,\n",
        "          \"reg_lambda\": 0.15,\n",
        "          \"min_child_weight\": 50,\n",
        "          \"bagging_seed\" : 420,\n",
        "          \"verbosity\" : -1\n",
        "         }\n",
        "lg_train = lgb.Dataset(X_train, label=y_train)\n",
        "lg_test = lgb.Dataset(X_test, label=y_test)\n",
        "model = lgb.train(params, lg_train, 1000, valid_sets=[lg_test], early_stopping_rounds=50, verbose_eval=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q89iDd5p7qTO"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLuolOWt7qTO"
      },
      "outputs": [],
      "source": [
        "y_pred[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7waMSvL7qTO"
      },
      "outputs": [],
      "source": [
        "# change y_pred format by select the index of maximum probability\n",
        "y_pred_list = [int(np.where(i == np.amax(i))[0]) for i in y_pred]\n",
        "y_pred_ar = np.array(y_pred_list)\n",
        "y_pred_ar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e66hdFjE7qTO"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred_ar))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "lqGcCv-17qTO"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "lgb.plot_importance(model, height=0.8, ax=ax)\n",
        "ax.grid(False)\n",
        "plt.ylabel('Feature', size=12)\n",
        "plt.xlabel('Importance', size=12)\n",
        "plt.title(\"Importance of the Features of our LightGBM Model\", fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QWSveLG7qTO"
      },
      "source": [
        "#### 2. Continent and Country-US"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZzUWrsWg7qTO"
      },
      "outputs": [],
      "source": [
        "ft2 = ft[['fullVisitorId','geoNetwork.country']]\n",
        "ft2['country_US']=0\n",
        "ft2.loc[ft2['geoNetwork.country']=='United States','country_US']=1\n",
        "ft2 = ft2.drop(['geoNetwork.country'],axis=1)\n",
        "\n",
        "ft2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deO2mfyS7qTO"
      },
      "outputs": [],
      "source": [
        "# check whether a customer only stay in US/not in US, but not for both\n",
        "us = ft2.groupby(['fullVisitorId']).agg(['sum'])\n",
        "us.columns = ['_'.join(col).strip() for col in us.columns.values]\n",
        "us = us.reset_index()\n",
        "\n",
        "us_merge = pd.merge(user_frequency, us, on='fullVisitorId')\n",
        "\n",
        "us_merge['us_only'] = (us_merge['Frequency']==us_merge['country_US_sum'])\n",
        "us_merge.loc[us_merge['Frequency']== 1,'us_only']=True\n",
        "us_merge.loc[us_merge['country_US_sum']== 0,'us_only']=True\n",
        "us_merge['us_only'] = us_merge['us_only']*1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcBBeIKE7qTO"
      },
      "outputs": [],
      "source": [
        "# number of customer have been both in US and not in US\n",
        "print(us_merge[us_merge['us_only']==0]['fullVisitorId'].count())\n",
        "\n",
        "# presentage of customer have been both in US and not in US\n",
        "print(us_merge[us_merge['us_only']==0]['fullVisitorId'].count()/us_merge.fullVisitorId.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vke6z5857qTO"
      },
      "source": [
        "Since majority of people (99.8%) will staying in US not not in US through all the trasaction, we can consider whether in US as a stable attribute for a customer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIOxidCQ7qTO"
      },
      "outputs": [],
      "source": [
        "ft2 = ft2.groupby(['fullVisitorId']).agg(['max'])\n",
        "ft2.columns = ['_'.join(col).strip() for col in ft2.columns.values]\n",
        "ft2 = ft2.reset_index()\n",
        "ft2.columns = ['fullVisitorId', 'country_US']\n",
        "\n",
        "ft2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaALjpYp7qTO"
      },
      "outputs": [],
      "source": [
        "# merge continent features with cluster\n",
        "ft2_df = pd.merge(ft1_df, ft2, on='fullVisitorId')\n",
        "ft2_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kugja6Vh7qTO"
      },
      "outputs": [],
      "source": [
        "X = ft2_df.drop(['LTVCluster','logRevenue_12','fullVisitorId'],axis=1)\n",
        "y = ft2_df['LTVCluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7sdK3np7qTO"
      },
      "outputs": [],
      "source": [
        "X_columns = ft2_df.drop(['LTVCluster','logRevenue_12','fullVisitorId'],axis=1).columns\n",
        "y_column = ['LTVCluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaNFBEun7qTO"
      },
      "outputs": [],
      "source": [
        "# using SMOTE for unbalanced data\n",
        "X, y= SMOTE().fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB9Aq1Bl7qTO"
      },
      "outputs": [],
      "source": [
        "# change X, y to dataframe so that we can keep column names\n",
        "tX = pd.DataFrame(X, columns=X_columns)\n",
        "ty = pd.DataFrame(y, columns=y_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pj-8_067qTP"
      },
      "outputs": [],
      "source": [
        "# take a look at y after SMOTE, 4 clusters have the same samples now\n",
        "ty.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdpDpQjH7qTP"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tX, ty, test_size=0.2, random_state=56)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41akGS3P7qTP"
      },
      "outputs": [],
      "source": [
        "# fitting lightGBM\n",
        "params = {\"objective\" : \"multiclass\",\n",
        "          \"num_class\": 4,\n",
        "          \"metric\" : \"multi_error\",\n",
        "          \"num_leaves\" : 30,\n",
        "          \"min_child_weight\" : 50,\n",
        "          \"learning_rate\" : 0.1,\n",
        "          \"bagging_fraction\" : 0.7,\n",
        "          \"feature_fraction\" : 0.7,\n",
        "          \"reg_alpha\": 0.15,\n",
        "          \"reg_lambda\": 0.15,\n",
        "          \"min_child_weight\": 50,\n",
        "          \"bagging_seed\" : 420,\n",
        "          \"verbosity\" : -1\n",
        "         }\n",
        "lg_train = lgb.Dataset(X_train, label=y_train)\n",
        "lg_test = lgb.Dataset(X_test, label=y_test)\n",
        "model = lgb.train(params, lg_train, 1000, valid_sets=[lg_test], early_stopping_rounds=50, verbose_eval=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHOdHtop7qTP"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGEkHMXj7qTP"
      },
      "outputs": [],
      "source": [
        "y_pred[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENDhttkM7qTP"
      },
      "outputs": [],
      "source": [
        "# change y_pred format by select the index of maximum probability\n",
        "y_pred_list = [int(np.where(i == np.amax(i))[0]) for i in y_pred]\n",
        "y_pred_ar = np.array(y_pred_list)\n",
        "y_pred_ar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qs_agpS7qTP"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred_ar))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Tv3VDDtY7qTP"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "lgb.plot_importance(model, height=0.8, ax=ax)\n",
        "ax.grid(False)\n",
        "plt.ylabel('Feature', size=12)\n",
        "plt.xlabel('Importance', size=12)\n",
        "plt.title(\"Importance of the Features of our LightGBM Model\", fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LTXOMBS7qTP"
      },
      "source": [
        "#### 3. Country-US Only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "XhpeGBSG7qTP"
      },
      "outputs": [],
      "source": [
        "# merge continent features with cluster\n",
        "ft_us = pd.merge(cluster, ft2, on='fullVisitorId')\n",
        "ft_us.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBy9_rF67qTP"
      },
      "outputs": [],
      "source": [
        "X = ft_us.drop(['LTVCluster','logRevenue_12','fullVisitorId'],axis=1)\n",
        "y = ft_us['LTVCluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAQYS1Re7qTP"
      },
      "outputs": [],
      "source": [
        "X_columns = ft_us.drop(['LTVCluster','logRevenue_12','fullVisitorId'],axis=1).columns\n",
        "y_column = ['LTVCluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNLU0Rqe7qTP"
      },
      "outputs": [],
      "source": [
        "# using SMOTE for unbalanced data\n",
        "X, y= SMOTE().fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlVQaAp_7qTP"
      },
      "outputs": [],
      "source": [
        "# change X, y to dataframe so that we can keep column names\n",
        "tX = pd.DataFrame(X, columns=X_columns)\n",
        "ty = pd.DataFrame(y, columns=y_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmEDOl_E7qTP"
      },
      "outputs": [],
      "source": [
        "# take a look at y after SMOTE, 4 clusters have the same samples now\n",
        "ty.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXRBSCN37qTP"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tX, ty, test_size=0.2, random_state=56)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Uluuq5a7qTP"
      },
      "outputs": [],
      "source": [
        "# fitting lightGBM\n",
        "params = {\"objective\" : \"multiclass\",\n",
        "          \"num_class\": 4,\n",
        "          \"metric\" : \"multi_error\",\n",
        "          \"num_leaves\" : 30,\n",
        "          \"min_child_weight\" : 50,\n",
        "          \"learning_rate\" : 0.1,\n",
        "          \"bagging_fraction\" : 0.7,\n",
        "          \"feature_fraction\" : 0.7,\n",
        "          \"reg_alpha\": 0.15,\n",
        "          \"reg_lambda\": 0.15,\n",
        "          \"min_child_weight\": 50,\n",
        "          \"bagging_seed\" : 420,\n",
        "          \"verbosity\" : -1\n",
        "         }\n",
        "lg_train = lgb.Dataset(X_train, label=y_train)\n",
        "lg_test = lgb.Dataset(X_test, label=y_test)\n",
        "model = lgb.train(params, lg_train, 1000, valid_sets=[lg_test], early_stopping_rounds=50, verbose_eval=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjVVd0tX7qTP"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCV_HjpV7qTP"
      },
      "outputs": [],
      "source": [
        "y_pred[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6GvQI0C7qTP"
      },
      "outputs": [],
      "source": [
        "# change y_pred format by select the index of maximum probability\n",
        "y_pred_list = [int(np.where(i == np.amax(i))[0]) for i in y_pred]\n",
        "y_pred_ar = np.array(y_pred_list)\n",
        "y_pred_ar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "qBZdGWIH7qTP"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred_ar))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vKzPVwcw7qTP"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "lgb.plot_importance(model, height=0.8, ax=ax)\n",
        "ax.grid(False)\n",
        "plt.ylabel('Feature', size=12)\n",
        "plt.xlabel('Importance', size=12)\n",
        "plt.title(\"Importance of the Features of our LightGBM Model\", fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTdvVqLS7qTP"
      },
      "source": [
        "Adding continent and country_us has the almost the same result as adding country_us only. To avoid overfitting, we will select the third model, adding contry_us only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vjxcjym7qTP"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "class_names=[0,1,2,3] # name  of classes\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = classes\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=0)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7ByxIux7qTQ"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred_ar, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred_ar, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjd_ml1671he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pziqrACS71v_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}